{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound Classification using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "* The following will demonstrate how to apply Deep Learning techniques to the classification of environmental sounds, specifically focusing on the identification of particular urban sounds.\n",
    "* When given an audio sample in a computer readable format (such as a .wav file) of a few seconds duration, we need to determine if it contains one of the target urban sounds with a corresponding Classification Accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset\n",
    "* For this we will use a dataset called Urbansound8K. The dataset contains 8732 sound excerpts (<=4s) of urban sounds from 10 classes, which are:\n",
    "    * Air Conditioner\n",
    "    * Car Horn\n",
    "    * Children Playing\n",
    "    * Dog bark\n",
    "    * Drilling\n",
    "    * Engine Idling\n",
    "    * Gun Shot\n",
    "    * Jackhammer\n",
    "    * Siren\n",
    "    * Street Music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing\n",
    "* Each audio file in the dataset will be used to extract an MFCC (meaning we have an image representation for each audio sample) and store it in a Panda Dataframe along with it’s classification label. \n",
    "* For this we will use Librosa’s mfcc() function which generates an MFCC from time series audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        #res_type -> resample type\n",
    "        #n_mfcc -> extracting features\n",
    "        #axis = 0 -> row wise\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)      \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file)\n",
    "        return None \n",
    "    return mfccsscaled\n",
    "    \n",
    "# Load various imports \n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# Set the path to full UrbanSound dataset \n",
    "fulldatasetpath = 'full_path_to/UrbanSound8K/audio/'\n",
    "metadata = pd.read_csv(fulldatasetpath + 'metadata/UrbanSound8K.csv')\n",
    "\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in metadata.iterrows():\n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    class_label = row[\"classID\"]\n",
    "    data = extract_features(file_name)\n",
    "    features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above code is used for data loading <br>\n",
    "> Each audio(.wav) file is converted to pandas dataframe using mfcc and mapped to respective class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[-215.793, 71.66612, -131.81377, -52.09133, -2...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[-424.68686, 110.562294, -54.14824, 62.01074, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[-459.56473, 122.80033, -47.924713, 53.265697,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature  class_label\n",
       "0  [-215.793, 71.66612, -131.81377, -52.09133, -2...            3\n",
       "1  [-424.68686, 110.562294, -54.14824, 62.01074, ...            2\n",
       "2  [-459.56473, 122.80033, -47.924713, 53.265697,...            2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To display first three rows (as example)\n",
    "featuresdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As we can observe that the audio files are converted a series of numerical data and mapped to respective labels and stored in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    1000\n",
      "5    1000\n",
      "4    1000\n",
      "3    1000\n",
      "2    1000\n",
      "9    1000\n",
      "0    1000\n",
      "8     929\n",
      "1     429\n",
      "6     374\n",
      "Name: class_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#To show how many audio files for the respective class with a bar plot representation along with count\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=featuresdf['class_label'], data = featuresdf)\n",
    "plt.show()\n",
    "\n",
    "print(featuresdf['class_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 80-20 Train Test split of Data for that we have used Scikit learn train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8732,)\n",
      "(6985, 40)\n",
      "(1747, 40)\n",
      "(6985, 10)\n",
      "(1747, 10)\n"
     ]
    }
   ],
   "source": [
    "#total number of dataset available\n",
    "print(featuresdf['class_label'].shape)\n",
    "\n",
    "#Display the number of row and number of column for the below shapes of X and Y\n",
    "#X\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "#Y\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.89295654e+01,  1.17308945e+02, -9.83756409e+01, -7.98500586e+00,\n",
       "       -3.02204723e+01,  1.68749981e+01,  1.71548309e+01,  3.09231224e+01,\n",
       "       -1.71333466e+01,  2.00548248e+01, -2.74858284e+00,  7.96005678e+00,\n",
       "       -9.57434750e+00, -1.44931259e+01,  1.03712463e+00,  1.80540237e+01,\n",
       "        1.84225464e+00,  1.44545555e+01, -5.32240629e+00,  7.84389162e+00,\n",
       "       -5.15528619e-01, -4.21097898e+00, -2.51499391e+00,  2.17578435e+00,\n",
       "        2.18523932e+00,  6.41926432e+00, -4.31260079e-01,  2.79778570e-01,\n",
       "        2.71853781e+00,  3.60929757e-01,  3.54210883e-01, -3.53838682e+00,\n",
       "        1.47081196e+00,  2.81663585e+00,  2.55088240e-01, -9.59454775e-02,\n",
       "       -3.50653887e-01,  2.18912864e+00,  5.62749243e+00, -3.90323305e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example for first row of Y Train as target variable (like One-Hot Encoding)\n",
    "print(y_train[0])\n",
    "\n",
    "#example for first row of X Train as Feature variable\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How data in each train and test set is represented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 9, 3, 16)          80        \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 9, 3, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 8, 2, 32)          2080      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 8, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 7, 1, 64)          8256      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 7, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 7, 1, 128)         8320      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 7, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 20,026\n",
      "Trainable params: 20,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model -> 1\n",
    "#Construct Sequential model\n",
    "#removed Max Pooling and hence Accuracy reduced by 0.20 approx (Not much of difference)\n",
    "#used Dropout=0.2 initially but achieved accuracy around 59% Hence, when tried with 0.5, it worked better\n",
    "#filter size starts from 16 and then after it was increased with multiple of 2 for its consecutive layers\n",
    "#kernel size with 1 was deviating too much while training\n",
    "#batch size = 52\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "#converting 40 features extracted from MFCC for each audio to represent in 2D Matrix (10*4=40)\n",
    "num_rows = 10\n",
    "n_columns = 4\n",
    "\n",
    "#Mono-Channel\n",
    "num_channels = 1\n",
    "\n",
    "#reshaping the numpy array (2D Matrix) to 10 * 4 * 1 from 8732 * 40\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows,n_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows,n_columns, num_channels)\n",
    "\n",
    "#reshaping label/target variable to column wise\n",
    "num_labels = yy.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows,n_columns, num_channels), activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=1, activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6985 samples, validate on 1747 samples\n",
      "Epoch 1/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 2.4185 - accuracy: 0.1416 - val_loss: 2.1772 - val_accuracy: 0.1883\n",
      "Epoch 2/100\n",
      "6985/6985 [==============================] - 6s 818us/step - loss: 2.1692 - accuracy: 0.1848 - val_loss: 2.0895 - val_accuracy: 0.2221\n",
      "Epoch 3/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 2.0994 - accuracy: 0.2127 - val_loss: 2.0245 - val_accuracy: 0.2793\n",
      "Epoch 4/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 2.0481 - accuracy: 0.2315 - val_loss: 1.9584 - val_accuracy: 0.3148\n",
      "Epoch 5/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.9912 - accuracy: 0.2560 - val_loss: 1.8825 - val_accuracy: 0.3761\n",
      "Epoch 6/100\n",
      "6985/6985 [==============================] - 7s 949us/step - loss: 1.9273 - accuracy: 0.2926 - val_loss: 1.8160 - val_accuracy: 0.3795\n",
      "Epoch 7/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.8916 - accuracy: 0.3092 - val_loss: 1.7579 - val_accuracy: 0.4150\n",
      "Epoch 8/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.8428 - accuracy: 0.3278 - val_loss: 1.7067 - val_accuracy: 0.4184\n",
      "Epoch 9/100\n",
      "6985/6985 [==============================] - 6s 911us/step - loss: 1.8079 - accuracy: 0.3440 - val_loss: 1.6489 - val_accuracy: 0.4230\n",
      "Epoch 10/100\n",
      "6985/6985 [==============================] - 6s 807us/step - loss: 1.7713 - accuracy: 0.3626 - val_loss: 1.6045 - val_accuracy: 0.4619\n",
      "Epoch 11/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.7438 - accuracy: 0.3666 - val_loss: 1.5588 - val_accuracy: 0.4511\n",
      "Epoch 12/100\n",
      "6985/6985 [==============================] - 6s 843us/step - loss: 1.7299 - accuracy: 0.3671 - val_loss: 1.5640 - val_accuracy: 0.4642\n",
      "Epoch 13/100\n",
      "6985/6985 [==============================] - 5s 769us/step - loss: 1.6846 - accuracy: 0.3964 - val_loss: 1.5189 - val_accuracy: 0.4785\n",
      "Epoch 14/100\n",
      "6985/6985 [==============================] - 7s 956us/step - loss: 1.6794 - accuracy: 0.3950 - val_loss: 1.5084 - val_accuracy: 0.4699\n",
      "Epoch 15/100\n",
      "6985/6985 [==============================] - 6s 893us/step - loss: 1.6610 - accuracy: 0.4057 - val_loss: 1.4710 - val_accuracy: 0.4803\n",
      "Epoch 16/100\n",
      "6985/6985 [==============================] - 7s 940us/step - loss: 1.6597 - accuracy: 0.4021 - val_loss: 1.4552 - val_accuracy: 0.5026\n",
      "Epoch 17/100\n",
      "6985/6985 [==============================] - 6s 843us/step - loss: 1.6339 - accuracy: 0.4082 - val_loss: 1.4661 - val_accuracy: 0.4883\n",
      "Epoch 18/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.6259 - accuracy: 0.4172 - val_loss: 1.4333 - val_accuracy: 0.5049\n",
      "Epoch 19/100\n",
      "6985/6985 [==============================] - 6s 873us/step - loss: 1.6100 - accuracy: 0.4299 - val_loss: 1.4222 - val_accuracy: 0.5083\n",
      "Epoch 20/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.6260 - accuracy: 0.4243 - val_loss: 1.4187 - val_accuracy: 0.5203\n",
      "Epoch 21/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.5850 - accuracy: 0.4311 - val_loss: 1.4009 - val_accuracy: 0.5289\n",
      "Epoch 22/100\n",
      "6985/6985 [==============================] - 7s 963us/step - loss: 1.5852 - accuracy: 0.4261 - val_loss: 1.3824 - val_accuracy: 0.5507\n",
      "Epoch 23/100\n",
      "6985/6985 [==============================] - 7s 977us/step - loss: 1.5647 - accuracy: 0.4407 - val_loss: 1.3867 - val_accuracy: 0.5306\n",
      "Epoch 24/100\n",
      "6985/6985 [==============================] - 6s 838us/step - loss: 1.5758 - accuracy: 0.4326 - val_loss: 1.3465 - val_accuracy: 0.5547\n",
      "Epoch 25/100\n",
      "6985/6985 [==============================] - 6s 905us/step - loss: 1.5596 - accuracy: 0.4397 - val_loss: 1.3575 - val_accuracy: 0.5438\n",
      "Epoch 26/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.5581 - accuracy: 0.4368 - val_loss: 1.3491 - val_accuracy: 0.5467\n",
      "Epoch 27/100\n",
      "6985/6985 [==============================] - 6s 923us/step - loss: 1.5506 - accuracy: 0.4424 - val_loss: 1.3439 - val_accuracy: 0.5444\n",
      "Epoch 28/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.5422 - accuracy: 0.4577 - val_loss: 1.3240 - val_accuracy: 0.5713\n",
      "Epoch 29/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.5423 - accuracy: 0.4435 - val_loss: 1.3262 - val_accuracy: 0.5667\n",
      "Epoch 30/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.5302 - accuracy: 0.4581 - val_loss: 1.3285 - val_accuracy: 0.5598\n",
      "Epoch 31/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.5327 - accuracy: 0.4634 - val_loss: 1.3222 - val_accuracy: 0.5758\n",
      "Epoch 32/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.5175 - accuracy: 0.4598 - val_loss: 1.3197 - val_accuracy: 0.5558\n",
      "Epoch 33/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.5248 - accuracy: 0.4614 - val_loss: 1.2938 - val_accuracy: 0.5758\n",
      "Epoch 34/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.5065 - accuracy: 0.4596 - val_loss: 1.2651 - val_accuracy: 0.5959\n",
      "Epoch 35/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.5029 - accuracy: 0.4641 - val_loss: 1.2819 - val_accuracy: 0.5718\n",
      "Epoch 36/100\n",
      "6985/6985 [==============================] - 7s 935us/step - loss: 1.5017 - accuracy: 0.4706 - val_loss: 1.2804 - val_accuracy: 0.5844\n",
      "Epoch 37/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.5020 - accuracy: 0.4714 - val_loss: 1.2580 - val_accuracy: 0.5873\n",
      "Epoch 38/100\n",
      "6985/6985 [==============================] - 7s 976us/step - loss: 1.4941 - accuracy: 0.4713 - val_loss: 1.2966 - val_accuracy: 0.5736\n",
      "Epoch 39/100\n",
      "6985/6985 [==============================] - 6s 891us/step - loss: 1.4928 - accuracy: 0.4674 - val_loss: 1.2442 - val_accuracy: 0.6010\n",
      "Epoch 40/100\n",
      "6985/6985 [==============================] - 7s 933us/step - loss: 1.4750 - accuracy: 0.4786 - val_loss: 1.2421 - val_accuracy: 0.5942\n",
      "Epoch 41/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.4759 - accuracy: 0.4691 - val_loss: 1.2302 - val_accuracy: 0.6119\n",
      "Epoch 42/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.4804 - accuracy: 0.4812 - val_loss: 1.2494 - val_accuracy: 0.5856\n",
      "Epoch 43/100\n",
      "6985/6985 [==============================] - 6s 806us/step - loss: 1.4782 - accuracy: 0.4827 - val_loss: 1.2065 - val_accuracy: 0.6033\n",
      "Epoch 44/100\n",
      "6985/6985 [==============================] - 7s 963us/step - loss: 1.4788 - accuracy: 0.4775 - val_loss: 1.2179 - val_accuracy: 0.6148\n",
      "Epoch 45/100\n",
      "6985/6985 [==============================] - 6s 885us/step - loss: 1.4636 - accuracy: 0.4845 - val_loss: 1.2087 - val_accuracy: 0.6216\n",
      "Epoch 46/100\n",
      "6985/6985 [==============================] - 6s 883us/step - loss: 1.4575 - accuracy: 0.4869 - val_loss: 1.2116 - val_accuracy: 0.6085\n",
      "Epoch 47/100\n",
      "6985/6985 [==============================] - 6s 824us/step - loss: 1.4468 - accuracy: 0.4883 - val_loss: 1.2002 - val_accuracy: 0.6199\n",
      "Epoch 48/100\n",
      "6985/6985 [==============================] - 6s 888us/step - loss: 1.4517 - accuracy: 0.4849 - val_loss: 1.2047 - val_accuracy: 0.6068\n",
      "Epoch 49/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.4675 - accuracy: 0.4845 - val_loss: 1.2283 - val_accuracy: 0.5947\n",
      "Epoch 50/100\n",
      "6985/6985 [==============================] - 7s 976us/step - loss: 1.4591 - accuracy: 0.4933 - val_loss: 1.1905 - val_accuracy: 0.6062\n",
      "Epoch 51/100\n",
      "6985/6985 [==============================] - 6s 805us/step - loss: 1.4481 - accuracy: 0.4876 - val_loss: 1.1962 - val_accuracy: 0.6131\n",
      "Epoch 52/100\n",
      "6985/6985 [==============================] - 6s 911us/step - loss: 1.4490 - accuracy: 0.4882 - val_loss: 1.1978 - val_accuracy: 0.6079\n",
      "Epoch 53/100\n",
      "6985/6985 [==============================] - 6s 895us/step - loss: 1.4365 - accuracy: 0.4921 - val_loss: 1.1904 - val_accuracy: 0.6211\n",
      "Epoch 54/100\n",
      "6985/6985 [==============================] - 7s 959us/step - loss: 1.4371 - accuracy: 0.4933 - val_loss: 1.1887 - val_accuracy: 0.6079\n",
      "Epoch 55/100\n",
      "6985/6985 [==============================] - 7s 953us/step - loss: 1.4454 - accuracy: 0.4859 - val_loss: 1.1944 - val_accuracy: 0.6193\n",
      "Epoch 56/100\n",
      "6985/6985 [==============================] - 6s 901us/step - loss: 1.4331 - accuracy: 0.4901 - val_loss: 1.1741 - val_accuracy: 0.6302\n",
      "Epoch 57/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.4402 - accuracy: 0.4936 - val_loss: 1.1748 - val_accuracy: 0.6325\n",
      "Epoch 58/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.4244 - accuracy: 0.4962 - val_loss: 1.1489 - val_accuracy: 0.6474\n",
      "Epoch 59/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.4341 - accuracy: 0.5016 - val_loss: 1.1580 - val_accuracy: 0.6428\n",
      "Epoch 60/100\n",
      "6985/6985 [==============================] - 6s 915us/step - loss: 1.4201 - accuracy: 0.4955 - val_loss: 1.1606 - val_accuracy: 0.6314\n",
      "Epoch 61/100\n",
      "6985/6985 [==============================] - 6s 865us/step - loss: 1.4273 - accuracy: 0.4969 - val_loss: 1.1541 - val_accuracy: 0.6331\n",
      "Epoch 62/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.4337 - accuracy: 0.5001 - val_loss: 1.1757 - val_accuracy: 0.6239\n",
      "Epoch 63/100\n",
      "6985/6985 [==============================] - 6s 909us/step - loss: 1.4320 - accuracy: 0.4956 - val_loss: 1.1695 - val_accuracy: 0.6262\n",
      "Epoch 64/100\n",
      "6985/6985 [==============================] - 5s 752us/step - loss: 1.4272 - accuracy: 0.4998 - val_loss: 1.1517 - val_accuracy: 0.6503\n",
      "Epoch 65/100\n",
      "6985/6985 [==============================] - 6s 929us/step - loss: 1.4223 - accuracy: 0.4982 - val_loss: 1.1675 - val_accuracy: 0.6211\n",
      "Epoch 66/100\n",
      "6985/6985 [==============================] - 6s 895us/step - loss: 1.4137 - accuracy: 0.5012 - val_loss: 1.1570 - val_accuracy: 0.6382\n",
      "Epoch 67/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.4165 - accuracy: 0.4988 - val_loss: 1.1361 - val_accuracy: 0.6314\n",
      "Epoch 68/100\n",
      "6985/6985 [==============================] - 6s 844us/step - loss: 1.4108 - accuracy: 0.5028 - val_loss: 1.1473 - val_accuracy: 0.6348\n",
      "Epoch 69/100\n",
      "6985/6985 [==============================] - 7s 949us/step - loss: 1.3983 - accuracy: 0.5021 - val_loss: 1.1085 - val_accuracy: 0.6371\n",
      "Epoch 70/100\n",
      "6985/6985 [==============================] - 6s 930us/step - loss: 1.4079 - accuracy: 0.5026 - val_loss: 1.1174 - val_accuracy: 0.6480\n",
      "Epoch 71/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.4217 - accuracy: 0.5022 - val_loss: 1.1529 - val_accuracy: 0.6348\n",
      "Epoch 72/100\n",
      "6985/6985 [==============================] - 6s 809us/step - loss: 1.4098 - accuracy: 0.5038 - val_loss: 1.1402 - val_accuracy: 0.6256\n",
      "Epoch 73/100\n",
      "6985/6985 [==============================] - 6s 879us/step - loss: 1.4282 - accuracy: 0.4966 - val_loss: 1.1236 - val_accuracy: 0.6354\n",
      "Epoch 74/100\n",
      "6985/6985 [==============================] - 7s 946us/step - loss: 1.4166 - accuracy: 0.5049 - val_loss: 1.1359 - val_accuracy: 0.6463\n",
      "Epoch 75/100\n",
      "6985/6985 [==============================] - 6s 897us/step - loss: 1.4167 - accuracy: 0.5001 - val_loss: 1.1289 - val_accuracy: 0.6422\n",
      "Epoch 76/100\n",
      "6985/6985 [==============================] - 6s 924us/step - loss: 1.4146 - accuracy: 0.4911 - val_loss: 1.1305 - val_accuracy: 0.6388\n",
      "Epoch 77/100\n",
      "6985/6985 [==============================] - 6s 893us/step - loss: 1.4182 - accuracy: 0.5035 - val_loss: 1.1300 - val_accuracy: 0.6422\n",
      "Epoch 78/100\n",
      "6985/6985 [==============================] - 6s 855us/step - loss: 1.4100 - accuracy: 0.5047 - val_loss: 1.1341 - val_accuracy: 0.6319\n",
      "Epoch 79/100\n",
      "6985/6985 [==============================] - 7s 941us/step - loss: 1.3976 - accuracy: 0.5089 - val_loss: 1.1012 - val_accuracy: 0.6485\n",
      "Epoch 80/100\n",
      "6985/6985 [==============================] - 7s 964us/step - loss: 1.3777 - accuracy: 0.5142 - val_loss: 1.1133 - val_accuracy: 0.6382\n",
      "Epoch 81/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.4088 - accuracy: 0.5079 - val_loss: 1.1130 - val_accuracy: 0.6411\n",
      "Epoch 82/100\n",
      "6985/6985 [==============================] - 7s 963us/step - loss: 1.3908 - accuracy: 0.5138 - val_loss: 1.1066 - val_accuracy: 0.6422\n",
      "Epoch 83/100\n",
      "6985/6985 [==============================] - 7s 934us/step - loss: 1.4068 - accuracy: 0.5037 - val_loss: 1.1103 - val_accuracy: 0.6468\n",
      "Epoch 84/100\n",
      "6985/6985 [==============================] - 7s 981us/step - loss: 1.3907 - accuracy: 0.5180 - val_loss: 1.1086 - val_accuracy: 0.6411\n",
      "Epoch 85/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.4070 - accuracy: 0.5048 - val_loss: 1.1102 - val_accuracy: 0.6440\n",
      "Epoch 86/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.3866 - accuracy: 0.5158 - val_loss: 1.0878 - val_accuracy: 0.6583\n",
      "Epoch 87/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.3879 - accuracy: 0.5201 - val_loss: 1.1080 - val_accuracy: 0.6485\n",
      "Epoch 88/100\n",
      "6985/6985 [==============================] - 7s 1ms/step - loss: 1.3895 - accuracy: 0.5132 - val_loss: 1.1044 - val_accuracy: 0.6405\n",
      "Epoch 89/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.3886 - accuracy: 0.5081 - val_loss: 1.1148 - val_accuracy: 0.6474\n",
      "Epoch 90/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.4003 - accuracy: 0.5130 - val_loss: 1.1132 - val_accuracy: 0.6451\n",
      "Epoch 91/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.3904 - accuracy: 0.5088 - val_loss: 1.1009 - val_accuracy: 0.6520\n",
      "Epoch 92/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.3990 - accuracy: 0.4999 - val_loss: 1.1332 - val_accuracy: 0.6342\n",
      "Epoch 93/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.3910 - accuracy: 0.5071 - val_loss: 1.1132 - val_accuracy: 0.6491\n",
      "Epoch 94/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.3946 - accuracy: 0.5114 - val_loss: 1.1110 - val_accuracy: 0.6451\n",
      "Epoch 95/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.3676 - accuracy: 0.5181 - val_loss: 1.0878 - val_accuracy: 0.6537\n",
      "Epoch 96/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.3800 - accuracy: 0.5185 - val_loss: 1.0867 - val_accuracy: 0.6451\n",
      "Epoch 97/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.3841 - accuracy: 0.5117 - val_loss: 1.0922 - val_accuracy: 0.6468\n",
      "Epoch 98/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.3774 - accuracy: 0.5142 - val_loss: 1.1005 - val_accuracy: 0.6411\n",
      "Epoch 99/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.3847 - accuracy: 0.5098 - val_loss: 1.1016 - val_accuracy: 0.6342\n",
      "Epoch 100/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.3911 - accuracy: 0.5105 - val_loss: 1.0756 - val_accuracy: 0.6480\n",
      "Training completed in time:  0:11:41.512341\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 52\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1747/1747 [==============================] - 0s 207us/step\n",
      "Testing accuracy: 64.7968%\n"
     ]
    }
   ],
   "source": [
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Testing accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 9, 3, 16)          80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 9, 3, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 9, 3, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 2, 32)          2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 8, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 8, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 1, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 7, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 7, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 7, 1, 128)         8320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 7, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 7, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 7, 1, 256)         33024     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 7, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 7, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 54,330\n",
      "Trainable params: 54,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model -> 2\n",
    "#Added an additional layer to make more deep neural network with Max Pooling and batch size = 32\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "num_rows = 10\n",
    "n_columns = 4\n",
    "num_channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows,n_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows,n_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "#filter_size = 2\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows,n_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6985 samples, validate on 1747 samples\n",
      "Epoch 1/100\n",
      "6985/6985 [==============================] - 23s 3ms/step - loss: 2.3264 - accuracy: 0.1248 - val_loss: 2.2404 - val_accuracy: 0.1374\n",
      "Epoch 2/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 2.1520 - accuracy: 0.1898 - val_loss: 2.0524 - val_accuracy: 0.2444\n",
      "Epoch 3/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 2.0411 - accuracy: 0.2351 - val_loss: 1.9463 - val_accuracy: 0.2713\n",
      "Epoch 4/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 1.9442 - accuracy: 0.2799 - val_loss: 1.8317 - val_accuracy: 0.3039\n",
      "Epoch 5/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.8810 - accuracy: 0.3059 - val_loss: 1.7840 - val_accuracy: 0.3509\n",
      "Epoch 6/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.8279 - accuracy: 0.3303 - val_loss: 1.7551 - val_accuracy: 0.3612\n",
      "Epoch 7/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.7924 - accuracy: 0.3384 - val_loss: 1.7010 - val_accuracy: 0.3875\n",
      "Epoch 8/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 1.7636 - accuracy: 0.3550 - val_loss: 1.6520 - val_accuracy: 0.3955\n",
      "Epoch 9/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.7397 - accuracy: 0.3606 - val_loss: 1.6397 - val_accuracy: 0.4047\n",
      "Epoch 10/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.7269 - accuracy: 0.3738 - val_loss: 1.6146 - val_accuracy: 0.4184\n",
      "Epoch 11/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.7000 - accuracy: 0.3790 - val_loss: 1.6303 - val_accuracy: 0.4076\n",
      "Epoch 12/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.6957 - accuracy: 0.3848 - val_loss: 1.6106 - val_accuracy: 0.4173\n",
      "Epoch 13/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.6723 - accuracy: 0.4019 - val_loss: 1.5239 - val_accuracy: 0.4568\n",
      "Epoch 14/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.6715 - accuracy: 0.3983 - val_loss: 1.5884 - val_accuracy: 0.4322\n",
      "Epoch 15/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.6567 - accuracy: 0.4056 - val_loss: 1.5363 - val_accuracy: 0.4574\n",
      "Epoch 16/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.6330 - accuracy: 0.4130 - val_loss: 1.4798 - val_accuracy: 0.4762\n",
      "Epoch 17/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.6223 - accuracy: 0.4156 - val_loss: 1.4535 - val_accuracy: 0.4860\n",
      "Epoch 18/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.6314 - accuracy: 0.4166 - val_loss: 1.5064 - val_accuracy: 0.4682\n",
      "Epoch 19/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.6192 - accuracy: 0.4222 - val_loss: 1.4836 - val_accuracy: 0.4619\n",
      "Epoch 20/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.6006 - accuracy: 0.4262 - val_loss: 1.4767 - val_accuracy: 0.4722\n",
      "Epoch 21/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.5859 - accuracy: 0.4283 - val_loss: 1.4543 - val_accuracy: 0.4865\n",
      "Epoch 22/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.6000 - accuracy: 0.4248 - val_loss: 1.4397 - val_accuracy: 0.5020\n",
      "Epoch 23/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.5829 - accuracy: 0.4354 - val_loss: 1.4586 - val_accuracy: 0.4888\n",
      "Epoch 24/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 1.5805 - accuracy: 0.4364 - val_loss: 1.4192 - val_accuracy: 0.4934\n",
      "Epoch 25/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 1.5715 - accuracy: 0.4418 - val_loss: 1.4368 - val_accuracy: 0.5175\n",
      "Epoch 26/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 1.5652 - accuracy: 0.4444 - val_loss: 1.4101 - val_accuracy: 0.5049\n",
      "Epoch 27/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.5610 - accuracy: 0.4418 - val_loss: 1.3965 - val_accuracy: 0.5140\n",
      "Epoch 28/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 1.5541 - accuracy: 0.4513 - val_loss: 1.3852 - val_accuracy: 0.5260\n",
      "Epoch 29/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.5479 - accuracy: 0.4450 - val_loss: 1.3733 - val_accuracy: 0.5415\n",
      "Epoch 30/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.5408 - accuracy: 0.4454 - val_loss: 1.4006 - val_accuracy: 0.4997\n",
      "Epoch 31/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.5446 - accuracy: 0.4534 - val_loss: 1.3668 - val_accuracy: 0.5363\n",
      "Epoch 32/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.5400 - accuracy: 0.4429 - val_loss: 1.3599 - val_accuracy: 0.5266\n",
      "Epoch 33/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.5214 - accuracy: 0.4498 - val_loss: 1.3076 - val_accuracy: 0.5730\n",
      "Epoch 34/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.5221 - accuracy: 0.4548 - val_loss: 1.3330 - val_accuracy: 0.5409\n",
      "Epoch 35/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.5194 - accuracy: 0.4650 - val_loss: 1.3719 - val_accuracy: 0.5072\n",
      "Epoch 36/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 1.5289 - accuracy: 0.4543 - val_loss: 1.2939 - val_accuracy: 0.5667\n",
      "Epoch 37/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.5193 - accuracy: 0.4586 - val_loss: 1.3200 - val_accuracy: 0.5564\n",
      "Epoch 38/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.5021 - accuracy: 0.4729 - val_loss: 1.3121 - val_accuracy: 0.5615\n",
      "Epoch 39/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.5040 - accuracy: 0.4641 - val_loss: 1.2990 - val_accuracy: 0.5684\n",
      "Epoch 40/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.5089 - accuracy: 0.4654 - val_loss: 1.3069 - val_accuracy: 0.5518\n",
      "Epoch 41/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.4914 - accuracy: 0.4709 - val_loss: 1.2659 - val_accuracy: 0.5982\n",
      "Epoch 42/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.4924 - accuracy: 0.4712 - val_loss: 1.2574 - val_accuracy: 0.5947\n",
      "Epoch 43/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4898 - accuracy: 0.4736 - val_loss: 1.3033 - val_accuracy: 0.5581\n",
      "Epoch 44/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.4913 - accuracy: 0.4691 - val_loss: 1.2659 - val_accuracy: 0.5713\n",
      "Epoch 45/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4882 - accuracy: 0.4677 - val_loss: 1.2666 - val_accuracy: 0.5804\n",
      "Epoch 46/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4902 - accuracy: 0.4743 - val_loss: 1.2492 - val_accuracy: 0.5959\n",
      "Epoch 47/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 1.4871 - accuracy: 0.4733 - val_loss: 1.2585 - val_accuracy: 0.5787\n",
      "Epoch 48/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 1.4748 - accuracy: 0.4810 - val_loss: 1.2379 - val_accuracy: 0.5924\n",
      "Epoch 49/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.4812 - accuracy: 0.4760 - val_loss: 1.3081 - val_accuracy: 0.5564\n",
      "Epoch 50/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 1.4720 - accuracy: 0.4714 - val_loss: 1.2693 - val_accuracy: 0.5695\n",
      "Epoch 51/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4691 - accuracy: 0.4754 - val_loss: 1.2563 - val_accuracy: 0.5850\n",
      "Epoch 52/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.4856 - accuracy: 0.4739 - val_loss: 1.2442 - val_accuracy: 0.6027\n",
      "Epoch 53/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4652 - accuracy: 0.4793 - val_loss: 1.2323 - val_accuracy: 0.6113\n",
      "Epoch 54/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4665 - accuracy: 0.4807 - val_loss: 1.2530 - val_accuracy: 0.5850\n",
      "Epoch 55/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4695 - accuracy: 0.4737 - val_loss: 1.2425 - val_accuracy: 0.6010\n",
      "Epoch 56/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.4604 - accuracy: 0.4775 - val_loss: 1.2232 - val_accuracy: 0.5993\n",
      "Epoch 57/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.4633 - accuracy: 0.4800 - val_loss: 1.2319 - val_accuracy: 0.6005\n",
      "Epoch 58/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4453 - accuracy: 0.4849 - val_loss: 1.2080 - val_accuracy: 0.5959\n",
      "Epoch 59/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 1.4658 - accuracy: 0.4895 - val_loss: 1.2397 - val_accuracy: 0.5924\n",
      "Epoch 60/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.4568 - accuracy: 0.4842 - val_loss: 1.2000 - val_accuracy: 0.6148\n",
      "Epoch 61/100\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 1.4468 - accuracy: 0.4858 - val_loss: 1.2324 - val_accuracy: 0.6027\n",
      "Epoch 62/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.4378 - accuracy: 0.4916 - val_loss: 1.2219 - val_accuracy: 0.6010\n",
      "Epoch 63/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4609 - accuracy: 0.4822 - val_loss: 1.2133 - val_accuracy: 0.6148\n",
      "Epoch 64/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 1.4412 - accuracy: 0.4938 - val_loss: 1.2219 - val_accuracy: 0.5999\n",
      "Epoch 65/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.4455 - accuracy: 0.4948 - val_loss: 1.2135 - val_accuracy: 0.6045\n",
      "Epoch 66/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.4519 - accuracy: 0.4909 - val_loss: 1.2446 - val_accuracy: 0.5850\n",
      "Epoch 67/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4366 - accuracy: 0.4946 - val_loss: 1.2128 - val_accuracy: 0.6039\n",
      "Epoch 68/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 1.4240 - accuracy: 0.4956 - val_loss: 1.2056 - val_accuracy: 0.6148\n",
      "Epoch 69/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4458 - accuracy: 0.4953 - val_loss: 1.2254 - val_accuracy: 0.6079\n",
      "Epoch 70/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4506 - accuracy: 0.4896 - val_loss: 1.2248 - val_accuracy: 0.6045\n",
      "Epoch 71/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.4470 - accuracy: 0.4858 - val_loss: 1.2107 - val_accuracy: 0.6027\n",
      "Epoch 72/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.4384 - accuracy: 0.4888 - val_loss: 1.2473 - val_accuracy: 0.5896\n",
      "Epoch 73/100\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 1.4412 - accuracy: 0.4869 - val_loss: 1.2019 - val_accuracy: 0.6090\n",
      "Epoch 74/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 1.4322 - accuracy: 0.4975 - val_loss: 1.1641 - val_accuracy: 0.6268\n",
      "Epoch 75/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.4243 - accuracy: 0.4912 - val_loss: 1.1984 - val_accuracy: 0.6211\n",
      "Epoch 76/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 1.4412 - accuracy: 0.4901 - val_loss: 1.1749 - val_accuracy: 0.6285\n",
      "Epoch 77/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 1.4374 - accuracy: 0.4880 - val_loss: 1.1914 - val_accuracy: 0.6142\n",
      "Epoch 78/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 1.4442 - accuracy: 0.4925 - val_loss: 1.1795 - val_accuracy: 0.6262\n",
      "Epoch 79/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4320 - accuracy: 0.4982 - val_loss: 1.1760 - val_accuracy: 0.6228\n",
      "Epoch 80/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 1.4429 - accuracy: 0.4918 - val_loss: 1.2020 - val_accuracy: 0.6251\n",
      "Epoch 81/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 1.4138 - accuracy: 0.5026 - val_loss: 1.1892 - val_accuracy: 0.6102\n",
      "Epoch 82/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 1.4374 - accuracy: 0.4936 - val_loss: 1.1992 - val_accuracy: 0.6142\n",
      "Epoch 83/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 1.4327 - accuracy: 0.4996 - val_loss: 1.1967 - val_accuracy: 0.6222\n",
      "Epoch 84/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 1.4334 - accuracy: 0.4981 - val_loss: 1.2413 - val_accuracy: 0.5924\n",
      "Epoch 85/100\n",
      "6985/6985 [==============================] - 19s 3ms/step - loss: 1.4182 - accuracy: 0.4956 - val_loss: 1.1911 - val_accuracy: 0.6171\n",
      "Epoch 86/100\n",
      "6985/6985 [==============================] - 20s 3ms/step - loss: 1.4245 - accuracy: 0.4994 - val_loss: 1.1808 - val_accuracy: 0.6302\n",
      "Epoch 87/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 1.4227 - accuracy: 0.5049 - val_loss: 1.1901 - val_accuracy: 0.6142\n",
      "Epoch 88/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 1.4264 - accuracy: 0.4996 - val_loss: 1.2202 - val_accuracy: 0.6027\n",
      "Epoch 89/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 1.4245 - accuracy: 0.5016 - val_loss: 1.2090 - val_accuracy: 0.6039\n",
      "Epoch 90/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 1.4211 - accuracy: 0.4962 - val_loss: 1.1502 - val_accuracy: 0.6331\n",
      "Epoch 91/100\n",
      "6985/6985 [==============================] - 19s 3ms/step - loss: 1.4255 - accuracy: 0.4912 - val_loss: 1.1617 - val_accuracy: 0.6337\n",
      "Epoch 92/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 1.4071 - accuracy: 0.5034 - val_loss: 1.1697 - val_accuracy: 0.6256\n",
      "Epoch 93/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 1.4146 - accuracy: 0.4961 - val_loss: 1.1778 - val_accuracy: 0.6193\n",
      "Epoch 94/100\n",
      "6985/6985 [==============================] - 19s 3ms/step - loss: 1.4294 - accuracy: 0.4974 - val_loss: 1.1658 - val_accuracy: 0.6211\n",
      "Epoch 95/100\n",
      "6985/6985 [==============================] - 17s 3ms/step - loss: 1.4235 - accuracy: 0.4992 - val_loss: 1.2231 - val_accuracy: 0.6056\n",
      "Epoch 96/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 1.4117 - accuracy: 0.4998 - val_loss: 1.1987 - val_accuracy: 0.6096\n",
      "Epoch 97/100\n",
      "6985/6985 [==============================] - 19s 3ms/step - loss: 1.4122 - accuracy: 0.5025 - val_loss: 1.1364 - val_accuracy: 0.6422\n",
      "Epoch 98/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 1.4368 - accuracy: 0.4929 - val_loss: 1.1755 - val_accuracy: 0.6245\n",
      "Epoch 99/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 1.4068 - accuracy: 0.5089 - val_loss: 1.1662 - val_accuracy: 0.6199\n",
      "Epoch 100/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 1.4210 - accuracy: 0.4984 - val_loss: 1.1772 - val_accuracy: 0.6251\n",
      "Training completed in time:  0:25:47.872283\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1747/1747 [==============================] - 1s 557us/step\n",
      "Testing accuracy: 62.5072%\n"
     ]
    }
   ],
   "source": [
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Testing accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So from Model 1 and Model 2, we understood that more the layers lesser the accuracy (Still Hypothesis, yet to prove) \n",
    "#But with more Batch Size (Still Hypothesis, yet to prove) and with MaxPooling it gave better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_139 (Conv2D)          (None, 9, 3, 32)          160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_104 (MaxPoolin (None, 9, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 9, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_140 (Conv2D)          (None, 8, 2, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_105 (MaxPoolin (None, 8, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 8, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_141 (Conv2D)          (None, 7, 1, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_106 (MaxPoolin (None, 7, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 7, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_142 (Conv2D)          (None, 7, 1, 256)         33024     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_107 (MaxPoolin (None, 7, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 7, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_14  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 76,906\n",
      "Trainable params: 76,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model -> 3\n",
    "# Same layers as Model 2 but only difference is in Batch Size = 64\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "num_rows = 10\n",
    "n_columns = 4\n",
    "num_channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows,n_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows,n_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, input_shape=(num_rows,n_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6985 samples, validate on 1747 samples\n",
      "Epoch 1/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 2.2375 - accuracy: 0.1741 - val_loss: 2.0272 - val_accuracy: 0.3045\n",
      "Epoch 2/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 2.0163 - accuracy: 0.2577 - val_loss: 1.8502 - val_accuracy: 0.3915\n",
      "Epoch 3/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.8643 - accuracy: 0.3164 - val_loss: 1.6695 - val_accuracy: 0.4259\n",
      "Epoch 4/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.7630 - accuracy: 0.3688 - val_loss: 1.5579 - val_accuracy: 0.4734\n",
      "Epoch 5/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.6984 - accuracy: 0.3923 - val_loss: 1.5073 - val_accuracy: 0.4946\n",
      "Epoch 6/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.6395 - accuracy: 0.4127 - val_loss: 1.4368 - val_accuracy: 0.5472\n",
      "Epoch 7/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.5961 - accuracy: 0.4228 - val_loss: 1.3892 - val_accuracy: 0.5638\n",
      "Epoch 8/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.5667 - accuracy: 0.4359 - val_loss: 1.3596 - val_accuracy: 0.5644\n",
      "Epoch 9/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.5092 - accuracy: 0.4654 - val_loss: 1.2976 - val_accuracy: 0.5764\n",
      "Epoch 10/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.4951 - accuracy: 0.4724 - val_loss: 1.2684 - val_accuracy: 0.5856\n",
      "Epoch 11/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.4681 - accuracy: 0.4843 - val_loss: 1.2228 - val_accuracy: 0.6090\n",
      "Epoch 12/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.4277 - accuracy: 0.4858 - val_loss: 1.2115 - val_accuracy: 0.5987\n",
      "Epoch 13/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.4438 - accuracy: 0.4836 - val_loss: 1.2039 - val_accuracy: 0.6159\n",
      "Epoch 14/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.3882 - accuracy: 0.5058 - val_loss: 1.1772 - val_accuracy: 0.6033\n",
      "Epoch 15/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.3621 - accuracy: 0.5248 - val_loss: 1.1598 - val_accuracy: 0.6165\n",
      "Epoch 16/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.3659 - accuracy: 0.5190 - val_loss: 1.1279 - val_accuracy: 0.6445\n",
      "Epoch 17/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.3491 - accuracy: 0.5233 - val_loss: 1.1186 - val_accuracy: 0.6422\n",
      "Epoch 18/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.3228 - accuracy: 0.5307 - val_loss: 1.0909 - val_accuracy: 0.6451\n",
      "Epoch 19/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.3162 - accuracy: 0.5380 - val_loss: 1.0872 - val_accuracy: 0.6520\n",
      "Epoch 20/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.3044 - accuracy: 0.5420 - val_loss: 1.0786 - val_accuracy: 0.6669\n",
      "Epoch 21/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.2876 - accuracy: 0.5550 - val_loss: 1.0715 - val_accuracy: 0.6606\n",
      "Epoch 22/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.2812 - accuracy: 0.5545 - val_loss: 1.0517 - val_accuracy: 0.6646\n",
      "Epoch 23/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.2724 - accuracy: 0.5578 - val_loss: 1.0329 - val_accuracy: 0.6720\n",
      "Epoch 24/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.2667 - accuracy: 0.5593 - val_loss: 1.0243 - val_accuracy: 0.6800\n",
      "Epoch 25/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.2535 - accuracy: 0.5625 - val_loss: 1.0257 - val_accuracy: 0.6743\n",
      "Epoch 26/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.2493 - accuracy: 0.5656 - val_loss: 0.9949 - val_accuracy: 0.6795\n",
      "Epoch 27/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.2216 - accuracy: 0.5722 - val_loss: 0.9511 - val_accuracy: 0.6915\n",
      "Epoch 28/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.2093 - accuracy: 0.5780 - val_loss: 0.9629 - val_accuracy: 0.6909\n",
      "Epoch 29/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.2268 - accuracy: 0.5761 - val_loss: 0.9747 - val_accuracy: 0.6823\n",
      "Epoch 30/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.2008 - accuracy: 0.5824 - val_loss: 0.9619 - val_accuracy: 0.6886\n",
      "Epoch 31/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.1937 - accuracy: 0.5877 - val_loss: 0.9310 - val_accuracy: 0.7001\n",
      "Epoch 32/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.1932 - accuracy: 0.5893 - val_loss: 0.9410 - val_accuracy: 0.6978\n",
      "Epoch 33/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.1853 - accuracy: 0.5953 - val_loss: 0.9516 - val_accuracy: 0.6926\n",
      "Epoch 34/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.1832 - accuracy: 0.5904 - val_loss: 0.9119 - val_accuracy: 0.7052\n",
      "Epoch 35/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.1765 - accuracy: 0.5916 - val_loss: 0.9008 - val_accuracy: 0.7109\n",
      "Epoch 36/100\n",
      "6985/6985 [==============================] - 10s 2ms/step - loss: 1.1601 - accuracy: 0.5997 - val_loss: 0.8861 - val_accuracy: 0.7201\n",
      "Epoch 37/100\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.1531 - accuracy: 0.5976 - val_loss: 0.9060 - val_accuracy: 0.7046\n",
      "Epoch 38/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.1672 - accuracy: 0.5956 - val_loss: 0.8918 - val_accuracy: 0.7109\n",
      "Epoch 39/100\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.1558 - accuracy: 0.5977 - val_loss: 0.8831 - val_accuracy: 0.7195\n",
      "Epoch 40/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.1492 - accuracy: 0.6043 - val_loss: 0.8928 - val_accuracy: 0.7293\n",
      "Epoch 41/100\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.1257 - accuracy: 0.6117 - val_loss: 0.8739 - val_accuracy: 0.7212\n",
      "Epoch 42/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.1370 - accuracy: 0.6037 - val_loss: 0.8692 - val_accuracy: 0.7144\n",
      "Epoch 43/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.1219 - accuracy: 0.6053 - val_loss: 0.8553 - val_accuracy: 0.7321\n",
      "Epoch 44/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.1109 - accuracy: 0.6216 - val_loss: 0.8644 - val_accuracy: 0.7235\n",
      "Epoch 45/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.1167 - accuracy: 0.6084 - val_loss: 0.8566 - val_accuracy: 0.7247\n",
      "Epoch 46/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.1216 - accuracy: 0.6155 - val_loss: 0.8451 - val_accuracy: 0.7230\n",
      "Epoch 47/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0965 - accuracy: 0.6190 - val_loss: 0.8303 - val_accuracy: 0.7384\n",
      "Epoch 48/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.1102 - accuracy: 0.6110 - val_loss: 0.8316 - val_accuracy: 0.7333\n",
      "Epoch 49/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.1012 - accuracy: 0.6176 - val_loss: 0.8405 - val_accuracy: 0.7378\n",
      "Epoch 50/100\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.0926 - accuracy: 0.6220 - val_loss: 0.8221 - val_accuracy: 0.7355\n",
      "Epoch 51/100\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.0988 - accuracy: 0.6206 - val_loss: 0.8316 - val_accuracy: 0.7424\n",
      "Epoch 52/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0910 - accuracy: 0.6249 - val_loss: 0.8250 - val_accuracy: 0.7418\n",
      "Epoch 53/100\n",
      "6985/6985 [==============================] - 10s 2ms/step - loss: 1.0850 - accuracy: 0.6246 - val_loss: 0.8105 - val_accuracy: 0.7527\n",
      "Epoch 54/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0734 - accuracy: 0.6375 - val_loss: 0.8028 - val_accuracy: 0.7562\n",
      "Epoch 55/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0807 - accuracy: 0.6358 - val_loss: 0.7875 - val_accuracy: 0.7539\n",
      "Epoch 56/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0891 - accuracy: 0.6346 - val_loss: 0.8021 - val_accuracy: 0.7493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.0767 - accuracy: 0.6342 - val_loss: 0.7966 - val_accuracy: 0.7436\n",
      "Epoch 58/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 1.0526 - accuracy: 0.6375 - val_loss: 0.7901 - val_accuracy: 0.7527\n",
      "Epoch 59/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.0641 - accuracy: 0.6354 - val_loss: 0.7751 - val_accuracy: 0.7550\n",
      "Epoch 60/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.0556 - accuracy: 0.6302 - val_loss: 0.7697 - val_accuracy: 0.7584\n",
      "Epoch 61/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.0466 - accuracy: 0.6366 - val_loss: 0.7769 - val_accuracy: 0.7550\n",
      "Epoch 62/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.0560 - accuracy: 0.6358 - val_loss: 0.7573 - val_accuracy: 0.7659\n",
      "Epoch 63/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 1.0420 - accuracy: 0.6429 - val_loss: 0.7681 - val_accuracy: 0.7630\n",
      "Epoch 64/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0324 - accuracy: 0.6497 - val_loss: 0.7598 - val_accuracy: 0.7624\n",
      "Epoch 65/100\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.0480 - accuracy: 0.6371 - val_loss: 0.7596 - val_accuracy: 0.7647\n",
      "Epoch 66/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0509 - accuracy: 0.6415 - val_loss: 0.7675 - val_accuracy: 0.7636\n",
      "Epoch 67/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0521 - accuracy: 0.6358 - val_loss: 0.7685 - val_accuracy: 0.7607\n",
      "Epoch 68/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0273 - accuracy: 0.6432 - val_loss: 0.7597 - val_accuracy: 0.7567\n",
      "Epoch 69/100\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.0187 - accuracy: 0.6485 - val_loss: 0.7199 - val_accuracy: 0.7785\n",
      "Epoch 70/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0487 - accuracy: 0.6408 - val_loss: 0.7470 - val_accuracy: 0.7670\n",
      "Epoch 71/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0180 - accuracy: 0.6474 - val_loss: 0.7592 - val_accuracy: 0.7636\n",
      "Epoch 72/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0286 - accuracy: 0.6359 - val_loss: 0.7412 - val_accuracy: 0.7779\n",
      "Epoch 73/100\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.0179 - accuracy: 0.6590 - val_loss: 0.7393 - val_accuracy: 0.7699\n",
      "Epoch 74/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0397 - accuracy: 0.6480 - val_loss: 0.7403 - val_accuracy: 0.7785\n",
      "Epoch 75/100\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.0372 - accuracy: 0.6372 - val_loss: 0.7435 - val_accuracy: 0.7647\n",
      "Epoch 76/100\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.0086 - accuracy: 0.6534 - val_loss: 0.7142 - val_accuracy: 0.7836\n",
      "Epoch 77/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0218 - accuracy: 0.6524 - val_loss: 0.7455 - val_accuracy: 0.7653\n",
      "Epoch 78/100\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.0283 - accuracy: 0.6521 - val_loss: 0.7245 - val_accuracy: 0.7705\n",
      "Epoch 79/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0195 - accuracy: 0.6503 - val_loss: 0.7341 - val_accuracy: 0.7842\n",
      "Epoch 80/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0047 - accuracy: 0.6508 - val_loss: 0.7191 - val_accuracy: 0.7750\n",
      "Epoch 81/100\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.0235 - accuracy: 0.6488 - val_loss: 0.7256 - val_accuracy: 0.7802\n",
      "Epoch 82/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0319 - accuracy: 0.6445 - val_loss: 0.7130 - val_accuracy: 0.7796\n",
      "Epoch 83/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0003 - accuracy: 0.6550 - val_loss: 0.7158 - val_accuracy: 0.7819\n",
      "Epoch 84/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0087 - accuracy: 0.6544 - val_loss: 0.7127 - val_accuracy: 0.7790\n",
      "Epoch 85/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0236 - accuracy: 0.6507 - val_loss: 0.7143 - val_accuracy: 0.7768\n",
      "Epoch 86/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0066 - accuracy: 0.6627 - val_loss: 0.7238 - val_accuracy: 0.7682\n",
      "Epoch 87/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0048 - accuracy: 0.6593 - val_loss: 0.7092 - val_accuracy: 0.7739\n",
      "Epoch 88/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 1.0089 - accuracy: 0.6568 - val_loss: 0.7007 - val_accuracy: 0.7853\n",
      "Epoch 89/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 0.9986 - accuracy: 0.6623 - val_loss: 0.7004 - val_accuracy: 0.7848\n",
      "Epoch 90/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 0.9991 - accuracy: 0.6606 - val_loss: 0.7232 - val_accuracy: 0.7785\n",
      "Epoch 91/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 0.9920 - accuracy: 0.6651 - val_loss: 0.7113 - val_accuracy: 0.7836\n",
      "Epoch 92/100\n",
      "6985/6985 [==============================] - 8s 1ms/step - loss: 0.9814 - accuracy: 0.6649 - val_loss: 0.7041 - val_accuracy: 0.7687\n",
      "Epoch 93/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 0.9979 - accuracy: 0.6600 - val_loss: 0.6777 - val_accuracy: 0.7985\n",
      "Epoch 94/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 0.9765 - accuracy: 0.6647 - val_loss: 0.7038 - val_accuracy: 0.7802\n",
      "Epoch 95/100\n",
      "6985/6985 [==============================] - 9s 1ms/step - loss: 0.9894 - accuracy: 0.6626 - val_loss: 0.6862 - val_accuracy: 0.8025\n",
      "Epoch 96/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 0.9842 - accuracy: 0.6667 - val_loss: 0.7031 - val_accuracy: 0.7750\n",
      "Epoch 97/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 0.9742 - accuracy: 0.6687 - val_loss: 0.6701 - val_accuracy: 0.7905\n",
      "Epoch 98/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 0.9666 - accuracy: 0.6646 - val_loss: 0.6702 - val_accuracy: 0.7882\n",
      "Epoch 99/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 0.9857 - accuracy: 0.6570 - val_loss: 0.6959 - val_accuracy: 0.7899\n",
      "Epoch 100/100\n",
      "6985/6985 [==============================] - 10s 1ms/step - loss: 0.9859 - accuracy: 0.6617 - val_loss: 0.7034 - val_accuracy: 0.7756\n",
      "Training completed in time:  0:16:22.498538\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 64\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1747/1747 [==============================] - 1s 740us/step\n",
      "Pre-training accuracy: 77.5615%\n"
     ]
    }
   ],
   "source": [
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from all the above three Models, we understood that more the Batch Size with use of Max Pooling and 0.5 as Dropout better the model is\n",
    "#Need to improve accuracy by breaking the hypothesis of deeper the neural network or wider the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_145 (Conv2D)          (None, 9, 3, 64)          320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_109 (MaxPoolin (None, 9, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 9, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_146 (Conv2D)          (None, 8, 2, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_110 (MaxPoolin (None, 8, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 8, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_147 (Conv2D)          (None, 7, 1, 256)         131328    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_111 (MaxPoolin (None, 7, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 7, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_15  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 167,114\n",
      "Trainable params: 167,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model -> 4\n",
    "# Construct model \n",
    "# Wider Neural Network with more hidden nodes than layers and more epochs = 125\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "num_rows = 10\n",
    "n_columns = 4\n",
    "num_channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows,n_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows,n_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, input_shape=(num_rows,n_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6985 samples, validate on 1747 samples\n",
      "Epoch 1/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 2.0288 - accuracy: 0.2676 - val_loss: 1.6618 - val_accuracy: 0.4385\n",
      "Epoch 2/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 1.6639 - accuracy: 0.4059 - val_loss: 1.4132 - val_accuracy: 0.5432\n",
      "Epoch 3/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.5281 - accuracy: 0.4590 - val_loss: 1.3096 - val_accuracy: 0.5638\n",
      "Epoch 4/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.4267 - accuracy: 0.4979 - val_loss: 1.2377 - val_accuracy: 0.5924\n",
      "Epoch 5/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.3624 - accuracy: 0.5203 - val_loss: 1.1294 - val_accuracy: 0.6325\n",
      "Epoch 6/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.2950 - accuracy: 0.5512 - val_loss: 1.1121 - val_accuracy: 0.6577\n",
      "Epoch 7/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.2527 - accuracy: 0.5702 - val_loss: 1.0407 - val_accuracy: 0.6783\n",
      "Epoch 8/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 1.2122 - accuracy: 0.5815 - val_loss: 0.9826 - val_accuracy: 0.7001\n",
      "Epoch 9/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.1753 - accuracy: 0.5980 - val_loss: 0.9312 - val_accuracy: 0.6932\n",
      "Epoch 10/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 1.1393 - accuracy: 0.6067 - val_loss: 0.9319 - val_accuracy: 0.7023\n",
      "Epoch 11/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.1191 - accuracy: 0.6122 - val_loss: 0.9011 - val_accuracy: 0.7247\n",
      "Epoch 12/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 1.0806 - accuracy: 0.6285 - val_loss: 0.8797 - val_accuracy: 0.7041\n",
      "Epoch 13/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 1.0634 - accuracy: 0.6359 - val_loss: 0.8358 - val_accuracy: 0.7527\n",
      "Epoch 14/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 1.0192 - accuracy: 0.6524 - val_loss: 0.8538 - val_accuracy: 0.7235\n",
      "Epoch 15/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.9994 - accuracy: 0.6578 - val_loss: 0.8053 - val_accuracy: 0.7390\n",
      "Epoch 16/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.9946 - accuracy: 0.6628 - val_loss: 0.7982 - val_accuracy: 0.7487\n",
      "Epoch 17/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.9568 - accuracy: 0.6795 - val_loss: 0.7531 - val_accuracy: 0.7624\n",
      "Epoch 18/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.9638 - accuracy: 0.6707 - val_loss: 0.7580 - val_accuracy: 0.7584\n",
      "Epoch 19/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.9418 - accuracy: 0.6714 - val_loss: 0.7252 - val_accuracy: 0.7716\n",
      "Epoch 20/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.9195 - accuracy: 0.6883 - val_loss: 0.6939 - val_accuracy: 0.7819\n",
      "Epoch 21/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.9035 - accuracy: 0.6928 - val_loss: 0.6783 - val_accuracy: 0.7831\n",
      "Epoch 22/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.8930 - accuracy: 0.6919 - val_loss: 0.6642 - val_accuracy: 0.7848\n",
      "Epoch 23/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.8761 - accuracy: 0.6999 - val_loss: 0.6668 - val_accuracy: 0.7836\n",
      "Epoch 24/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.8704 - accuracy: 0.7057 - val_loss: 0.6687 - val_accuracy: 0.7836\n",
      "Epoch 25/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.8294 - accuracy: 0.7144 - val_loss: 0.6584 - val_accuracy: 0.7928\n",
      "Epoch 26/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.8390 - accuracy: 0.7145 - val_loss: 0.6302 - val_accuracy: 0.7997\n",
      "Epoch 27/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.8322 - accuracy: 0.7127 - val_loss: 0.6247 - val_accuracy: 0.8025\n",
      "Epoch 28/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.8189 - accuracy: 0.7178 - val_loss: 0.6197 - val_accuracy: 0.8042\n",
      "Epoch 29/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.8090 - accuracy: 0.7309 - val_loss: 0.5960 - val_accuracy: 0.8163\n",
      "Epoch 30/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.7843 - accuracy: 0.7296 - val_loss: 0.6073 - val_accuracy: 0.7991\n",
      "Epoch 31/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.7695 - accuracy: 0.7360 - val_loss: 0.5783 - val_accuracy: 0.8180\n",
      "Epoch 32/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.7663 - accuracy: 0.7360 - val_loss: 0.5666 - val_accuracy: 0.8208\n",
      "Epoch 33/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.7869 - accuracy: 0.7351 - val_loss: 0.5812 - val_accuracy: 0.8163\n",
      "Epoch 34/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.7610 - accuracy: 0.7369 - val_loss: 0.5613 - val_accuracy: 0.8317\n",
      "Epoch 35/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.7437 - accuracy: 0.7417 - val_loss: 0.5635 - val_accuracy: 0.8197\n",
      "Epoch 36/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.7458 - accuracy: 0.7508 - val_loss: 0.5424 - val_accuracy: 0.8288\n",
      "Epoch 37/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.7438 - accuracy: 0.7460 - val_loss: 0.5394 - val_accuracy: 0.8283\n",
      "Epoch 38/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.7161 - accuracy: 0.7553 - val_loss: 0.5322 - val_accuracy: 0.8317\n",
      "Epoch 39/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.7255 - accuracy: 0.7585 - val_loss: 0.5323 - val_accuracy: 0.8317\n",
      "Epoch 40/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.7125 - accuracy: 0.7579 - val_loss: 0.5493 - val_accuracy: 0.8180\n",
      "Epoch 41/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.7208 - accuracy: 0.7562 - val_loss: 0.5376 - val_accuracy: 0.8323\n",
      "Epoch 42/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.7079 - accuracy: 0.7595 - val_loss: 0.5145 - val_accuracy: 0.8380\n",
      "Epoch 43/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6910 - accuracy: 0.7636 - val_loss: 0.5012 - val_accuracy: 0.8432\n",
      "Epoch 44/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6961 - accuracy: 0.7649 - val_loss: 0.4976 - val_accuracy: 0.8414\n",
      "Epoch 45/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6863 - accuracy: 0.7621 - val_loss: 0.5033 - val_accuracy: 0.8409\n",
      "Epoch 46/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6798 - accuracy: 0.7705 - val_loss: 0.4932 - val_accuracy: 0.8437\n",
      "Epoch 47/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6757 - accuracy: 0.7661 - val_loss: 0.4754 - val_accuracy: 0.8506\n",
      "Epoch 48/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6755 - accuracy: 0.7632 - val_loss: 0.4992 - val_accuracy: 0.8495\n",
      "Epoch 49/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6739 - accuracy: 0.7701 - val_loss: 0.4857 - val_accuracy: 0.8575\n",
      "Epoch 50/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6512 - accuracy: 0.7785 - val_loss: 0.4905 - val_accuracy: 0.8506\n",
      "Epoch 51/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6526 - accuracy: 0.7752 - val_loss: 0.4844 - val_accuracy: 0.8535\n",
      "Epoch 52/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6547 - accuracy: 0.7778 - val_loss: 0.4642 - val_accuracy: 0.8615\n",
      "Epoch 53/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6537 - accuracy: 0.7772 - val_loss: 0.4809 - val_accuracy: 0.8529\n",
      "Epoch 54/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6430 - accuracy: 0.7791 - val_loss: 0.4552 - val_accuracy: 0.8638\n",
      "Epoch 55/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.6302 - accuracy: 0.7830 - val_loss: 0.4534 - val_accuracy: 0.8592\n",
      "Epoch 56/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.6279 - accuracy: 0.7855 - val_loss: 0.4594 - val_accuracy: 0.8598\n",
      "Epoch 57/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.6551 - accuracy: 0.7791 - val_loss: 0.4603 - val_accuracy: 0.8632\n",
      "Epoch 58/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.6356 - accuracy: 0.7792 - val_loss: 0.4614 - val_accuracy: 0.8563\n",
      "Epoch 59/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.6404 - accuracy: 0.7811 - val_loss: 0.4528 - val_accuracy: 0.8620\n",
      "Epoch 60/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.6164 - accuracy: 0.7864 - val_loss: 0.4467 - val_accuracy: 0.8615\n",
      "Epoch 61/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.6178 - accuracy: 0.7822 - val_loss: 0.4459 - val_accuracy: 0.8638\n",
      "Epoch 62/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.5995 - accuracy: 0.7930 - val_loss: 0.4547 - val_accuracy: 0.8586\n",
      "Epoch 63/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.6219 - accuracy: 0.7812 - val_loss: 0.4330 - val_accuracy: 0.8666\n",
      "Epoch 64/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.6146 - accuracy: 0.7901 - val_loss: 0.4278 - val_accuracy: 0.8615\n",
      "Epoch 65/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5948 - accuracy: 0.7953 - val_loss: 0.4191 - val_accuracy: 0.8712\n",
      "Epoch 66/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.6038 - accuracy: 0.7918 - val_loss: 0.4376 - val_accuracy: 0.8655\n",
      "Epoch 67/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.6024 - accuracy: 0.7908 - val_loss: 0.4281 - val_accuracy: 0.8626\n",
      "Epoch 68/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5912 - accuracy: 0.7914 - val_loss: 0.4173 - val_accuracy: 0.8661\n",
      "Epoch 69/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5972 - accuracy: 0.7913 - val_loss: 0.4271 - val_accuracy: 0.8615\n",
      "Epoch 70/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5945 - accuracy: 0.7927 - val_loss: 0.4353 - val_accuracy: 0.8632\n",
      "Epoch 71/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5897 - accuracy: 0.7961 - val_loss: 0.4182 - val_accuracy: 0.8683\n",
      "Epoch 72/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.5770 - accuracy: 0.8053 - val_loss: 0.4228 - val_accuracy: 0.8661\n",
      "Epoch 73/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5778 - accuracy: 0.8023 - val_loss: 0.4190 - val_accuracy: 0.8683\n",
      "Epoch 74/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5673 - accuracy: 0.7997 - val_loss: 0.4062 - val_accuracy: 0.8775\n",
      "Epoch 75/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5738 - accuracy: 0.8044 - val_loss: 0.4107 - val_accuracy: 0.8741\n",
      "Epoch 76/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5767 - accuracy: 0.8029 - val_loss: 0.4076 - val_accuracy: 0.8752\n",
      "Epoch 77/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.5816 - accuracy: 0.7946 - val_loss: 0.3997 - val_accuracy: 0.8764\n",
      "Epoch 78/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5683 - accuracy: 0.8049 - val_loss: 0.4028 - val_accuracy: 0.8683\n",
      "Epoch 79/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.5748 - accuracy: 0.8016 - val_loss: 0.4037 - val_accuracy: 0.8746\n",
      "Epoch 80/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.5652 - accuracy: 0.8040 - val_loss: 0.4084 - val_accuracy: 0.8781\n",
      "Epoch 81/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5704 - accuracy: 0.8086 - val_loss: 0.4001 - val_accuracy: 0.8775\n",
      "Epoch 82/125\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 0.5695 - accuracy: 0.8084 - val_loss: 0.4006 - val_accuracy: 0.8769\n",
      "Epoch 83/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.5475 - accuracy: 0.8125 - val_loss: 0.4015 - val_accuracy: 0.8758\n",
      "Epoch 84/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5658 - accuracy: 0.8042 - val_loss: 0.3930 - val_accuracy: 0.8786\n",
      "Epoch 85/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5574 - accuracy: 0.8074 - val_loss: 0.3926 - val_accuracy: 0.8769\n",
      "Epoch 86/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5449 - accuracy: 0.8156 - val_loss: 0.3958 - val_accuracy: 0.8786\n",
      "Epoch 87/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5641 - accuracy: 0.8089 - val_loss: 0.4087 - val_accuracy: 0.8706\n",
      "Epoch 88/125\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 0.5403 - accuracy: 0.8170 - val_loss: 0.3914 - val_accuracy: 0.8741\n",
      "Epoch 89/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5537 - accuracy: 0.8106 - val_loss: 0.3921 - val_accuracy: 0.8741\n",
      "Epoch 90/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5468 - accuracy: 0.8135 - val_loss: 0.3958 - val_accuracy: 0.8712\n",
      "Epoch 91/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5635 - accuracy: 0.8024 - val_loss: 0.3896 - val_accuracy: 0.8798\n",
      "Epoch 92/125\n",
      "6985/6985 [==============================] - 12s 2ms/step - loss: 0.5493 - accuracy: 0.8125 - val_loss: 0.3929 - val_accuracy: 0.8849\n",
      "Epoch 93/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5296 - accuracy: 0.8135 - val_loss: 0.3818 - val_accuracy: 0.8764\n",
      "Epoch 94/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5393 - accuracy: 0.8137 - val_loss: 0.3848 - val_accuracy: 0.8815\n",
      "Epoch 95/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5252 - accuracy: 0.8206 - val_loss: 0.3817 - val_accuracy: 0.8821\n",
      "Epoch 96/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5247 - accuracy: 0.8117 - val_loss: 0.3897 - val_accuracy: 0.8798\n",
      "Epoch 97/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5403 - accuracy: 0.8139 - val_loss: 0.3815 - val_accuracy: 0.8781\n",
      "Epoch 98/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5213 - accuracy: 0.8258 - val_loss: 0.3945 - val_accuracy: 0.8764\n",
      "Epoch 99/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5328 - accuracy: 0.8137 - val_loss: 0.3847 - val_accuracy: 0.8769\n",
      "Epoch 100/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5177 - accuracy: 0.8188 - val_loss: 0.3775 - val_accuracy: 0.8792\n",
      "Epoch 101/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5186 - accuracy: 0.8245 - val_loss: 0.3730 - val_accuracy: 0.8786\n",
      "Epoch 102/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5256 - accuracy: 0.8220 - val_loss: 0.3716 - val_accuracy: 0.8809\n",
      "Epoch 103/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5326 - accuracy: 0.8168 - val_loss: 0.3831 - val_accuracy: 0.8804\n",
      "Epoch 104/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5206 - accuracy: 0.8210 - val_loss: 0.3722 - val_accuracy: 0.8867\n",
      "Epoch 105/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5044 - accuracy: 0.8232 - val_loss: 0.3750 - val_accuracy: 0.8867\n",
      "Epoch 106/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5065 - accuracy: 0.8215 - val_loss: 0.3724 - val_accuracy: 0.8861\n",
      "Epoch 107/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5221 - accuracy: 0.8175 - val_loss: 0.3695 - val_accuracy: 0.8838\n",
      "Epoch 108/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5067 - accuracy: 0.8259 - val_loss: 0.3927 - val_accuracy: 0.8752\n",
      "Epoch 109/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5074 - accuracy: 0.8189 - val_loss: 0.3661 - val_accuracy: 0.8872\n",
      "Epoch 110/125\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 0.5135 - accuracy: 0.8242 - val_loss: 0.3669 - val_accuracy: 0.8838\n",
      "Epoch 111/125\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5233 - accuracy: 0.8192 - val_loss: 0.3726 - val_accuracy: 0.8827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.4962 - accuracy: 0.8263 - val_loss: 0.3649 - val_accuracy: 0.8872\n",
      "Epoch 113/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5199 - accuracy: 0.8258 - val_loss: 0.3585 - val_accuracy: 0.8861\n",
      "Epoch 114/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5143 - accuracy: 0.8196 - val_loss: 0.3613 - val_accuracy: 0.8901\n",
      "Epoch 115/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5127 - accuracy: 0.8245 - val_loss: 0.3630 - val_accuracy: 0.8890\n",
      "Epoch 116/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5123 - accuracy: 0.8239 - val_loss: 0.3544 - val_accuracy: 0.8975\n",
      "Epoch 117/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5038 - accuracy: 0.8286 - val_loss: 0.3689 - val_accuracy: 0.8849\n",
      "Epoch 118/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5055 - accuracy: 0.8218 - val_loss: 0.3570 - val_accuracy: 0.8867\n",
      "Epoch 119/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5168 - accuracy: 0.8210 - val_loss: 0.3642 - val_accuracy: 0.8918\n",
      "Epoch 120/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5047 - accuracy: 0.8229 - val_loss: 0.3458 - val_accuracy: 0.8924\n",
      "Epoch 121/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5031 - accuracy: 0.8318 - val_loss: 0.3549 - val_accuracy: 0.8907\n",
      "Epoch 122/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.4863 - accuracy: 0.8276 - val_loss: 0.3547 - val_accuracy: 0.8901\n",
      "Epoch 123/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.5022 - accuracy: 0.8268 - val_loss: 0.3652 - val_accuracy: 0.8815\n",
      "Epoch 124/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.4939 - accuracy: 0.8324 - val_loss: 0.3542 - val_accuracy: 0.8890\n",
      "Epoch 125/125\n",
      "6985/6985 [==============================] - 11s 2ms/step - loss: 0.4999 - accuracy: 0.8299 - val_loss: 0.3541 - val_accuracy: 0.8901\n",
      "Training completed in time:  0:25:21.179704\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 125\n",
    "num_batch_size = 64\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    \n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1747/1747 [==============================] - 2s 897us/step\n",
      "Pre-training accuracy: 89.0097%\n"
     ]
    }
   ],
   "source": [
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 9, 3, 64)          320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 9, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 9, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 2, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 8, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 7, 1, 256)         131328    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 7, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 7, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 1, 512)         131584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 7, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 7, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 301,258\n",
      "Trainable params: 301,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model -> 5\n",
    "#Construct model \n",
    "#wider as well deeper neural network with epochs = 100\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "num_rows = 10\n",
    "n_columns = 4\n",
    "num_channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows,n_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows,n_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, input_shape=(num_rows,n_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6985 samples, validate on 1747 samples\n",
      "Epoch 1/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 2.1027 - accuracy: 0.2319 - val_loss: 1.8318 - val_accuracy: 0.3595\n",
      "Epoch 2/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.8079 - accuracy: 0.3429 - val_loss: 1.5475 - val_accuracy: 0.4837\n",
      "Epoch 3/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.6284 - accuracy: 0.4239 - val_loss: 1.3779 - val_accuracy: 0.5226\n",
      "Epoch 4/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.5272 - accuracy: 0.4601 - val_loss: 1.3180 - val_accuracy: 0.5444\n",
      "Epoch 5/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.4583 - accuracy: 0.4921 - val_loss: 1.2130 - val_accuracy: 0.6056\n",
      "Epoch 6/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.3791 - accuracy: 0.5214 - val_loss: 1.1817 - val_accuracy: 0.6062\n",
      "Epoch 7/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.3435 - accuracy: 0.5287 - val_loss: 1.1251 - val_accuracy: 0.6228\n",
      "Epoch 8/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.2814 - accuracy: 0.5532 - val_loss: 1.0695 - val_accuracy: 0.6548\n",
      "Epoch 9/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.2509 - accuracy: 0.5694 - val_loss: 1.0468 - val_accuracy: 0.6503\n",
      "Epoch 10/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.2325 - accuracy: 0.5767 - val_loss: 1.0223 - val_accuracy: 0.6663\n",
      "Epoch 11/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.1680 - accuracy: 0.6007 - val_loss: 0.9749 - val_accuracy: 0.6857\n",
      "Epoch 12/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.1411 - accuracy: 0.6084 - val_loss: 0.9572 - val_accuracy: 0.7001\n",
      "Epoch 13/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.1227 - accuracy: 0.6069 - val_loss: 0.8968 - val_accuracy: 0.7149\n",
      "Epoch 14/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.0901 - accuracy: 0.6186 - val_loss: 0.8598 - val_accuracy: 0.7167\n",
      "Epoch 15/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.0636 - accuracy: 0.6374 - val_loss: 0.8381 - val_accuracy: 0.7304\n",
      "Epoch 16/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 1.0405 - accuracy: 0.6432 - val_loss: 0.8278 - val_accuracy: 0.7321\n",
      "Epoch 17/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.0124 - accuracy: 0.6461 - val_loss: 0.8092 - val_accuracy: 0.7464\n",
      "Epoch 18/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 1.0047 - accuracy: 0.6590 - val_loss: 0.8200 - val_accuracy: 0.7321\n",
      "Epoch 19/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.9854 - accuracy: 0.6670 - val_loss: 0.7560 - val_accuracy: 0.7613\n",
      "Epoch 20/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.9562 - accuracy: 0.6760 - val_loss: 0.7705 - val_accuracy: 0.7516\n",
      "Epoch 21/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.9575 - accuracy: 0.6694 - val_loss: 0.7259 - val_accuracy: 0.7785\n",
      "Epoch 22/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.9319 - accuracy: 0.6799 - val_loss: 0.7022 - val_accuracy: 0.7722\n",
      "Epoch 23/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.9187 - accuracy: 0.6902 - val_loss: 0.6907 - val_accuracy: 0.7825\n",
      "Epoch 24/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.9017 - accuracy: 0.6935 - val_loss: 0.6759 - val_accuracy: 0.7968\n",
      "Epoch 25/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.8926 - accuracy: 0.6938 - val_loss: 0.6673 - val_accuracy: 0.7916\n",
      "Epoch 26/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.8691 - accuracy: 0.7042 - val_loss: 0.6657 - val_accuracy: 0.7894\n",
      "Epoch 27/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.8647 - accuracy: 0.6991 - val_loss: 0.6610 - val_accuracy: 0.7853\n",
      "Epoch 28/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.8519 - accuracy: 0.7071 - val_loss: 0.6396 - val_accuracy: 0.7928\n",
      "Epoch 29/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.8582 - accuracy: 0.7055 - val_loss: 0.6419 - val_accuracy: 0.7991\n",
      "Epoch 30/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.8205 - accuracy: 0.7208 - val_loss: 0.5929 - val_accuracy: 0.8208\n",
      "Epoch 31/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.8014 - accuracy: 0.7264 - val_loss: 0.5930 - val_accuracy: 0.8174\n",
      "Epoch 32/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.8150 - accuracy: 0.7247 - val_loss: 0.5907 - val_accuracy: 0.8088\n",
      "Epoch 33/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.8095 - accuracy: 0.7228 - val_loss: 0.5734 - val_accuracy: 0.8168\n",
      "Epoch 34/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.7955 - accuracy: 0.7270 - val_loss: 0.5919 - val_accuracy: 0.8134\n",
      "Epoch 35/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.7923 - accuracy: 0.7340 - val_loss: 0.5852 - val_accuracy: 0.8168\n",
      "Epoch 36/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.7863 - accuracy: 0.7237 - val_loss: 0.5706 - val_accuracy: 0.8174\n",
      "Epoch 37/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.7991 - accuracy: 0.7309 - val_loss: 0.5759 - val_accuracy: 0.8088\n",
      "Epoch 38/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.7756 - accuracy: 0.7350 - val_loss: 0.5683 - val_accuracy: 0.8191\n",
      "Epoch 39/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.7577 - accuracy: 0.7414 - val_loss: 0.5494 - val_accuracy: 0.8243\n",
      "Epoch 40/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.7505 - accuracy: 0.7393 - val_loss: 0.5445 - val_accuracy: 0.8231\n",
      "Epoch 41/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.7475 - accuracy: 0.7443 - val_loss: 0.5583 - val_accuracy: 0.8191\n",
      "Epoch 42/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.7420 - accuracy: 0.7457 - val_loss: 0.5385 - val_accuracy: 0.8277\n",
      "Epoch 43/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.7416 - accuracy: 0.7479 - val_loss: 0.5157 - val_accuracy: 0.8512\n",
      "Epoch 44/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.7189 - accuracy: 0.7540 - val_loss: 0.5212 - val_accuracy: 0.8346\n",
      "Epoch 45/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.7322 - accuracy: 0.7503 - val_loss: 0.5180 - val_accuracy: 0.8449\n",
      "Epoch 46/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.7040 - accuracy: 0.7583 - val_loss: 0.5188 - val_accuracy: 0.8386\n",
      "Epoch 47/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.7190 - accuracy: 0.7523 - val_loss: 0.5166 - val_accuracy: 0.8380\n",
      "Epoch 48/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.6978 - accuracy: 0.7585 - val_loss: 0.5151 - val_accuracy: 0.8374\n",
      "Epoch 49/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.7147 - accuracy: 0.7645 - val_loss: 0.5065 - val_accuracy: 0.8460\n",
      "Epoch 50/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.6994 - accuracy: 0.7644 - val_loss: 0.4880 - val_accuracy: 0.8495\n",
      "Epoch 51/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.6989 - accuracy: 0.7559 - val_loss: 0.5018 - val_accuracy: 0.8466\n",
      "Epoch 52/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.6959 - accuracy: 0.7569 - val_loss: 0.4895 - val_accuracy: 0.8517\n",
      "Epoch 53/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 0.6881 - accuracy: 0.7692 - val_loss: 0.5501 - val_accuracy: 0.8334\n",
      "Epoch 54/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 0.6915 - accuracy: 0.7641 - val_loss: 0.4862 - val_accuracy: 0.8483\n",
      "Epoch 55/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.6816 - accuracy: 0.7695 - val_loss: 0.4890 - val_accuracy: 0.8472\n",
      "Epoch 56/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.6729 - accuracy: 0.7735 - val_loss: 0.4967 - val_accuracy: 0.8443\n",
      "Epoch 57/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.6599 - accuracy: 0.7764 - val_loss: 0.4753 - val_accuracy: 0.8569\n",
      "Epoch 58/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.6750 - accuracy: 0.7705 - val_loss: 0.4641 - val_accuracy: 0.8632\n",
      "Epoch 59/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.6628 - accuracy: 0.7725 - val_loss: 0.4861 - val_accuracy: 0.8495\n",
      "Epoch 60/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.6663 - accuracy: 0.7712 - val_loss: 0.4690 - val_accuracy: 0.8575\n",
      "Epoch 61/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.6525 - accuracy: 0.7814 - val_loss: 0.4685 - val_accuracy: 0.8535\n",
      "Epoch 62/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.6565 - accuracy: 0.7749 - val_loss: 0.4721 - val_accuracy: 0.8495\n",
      "Epoch 63/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.6356 - accuracy: 0.7857 - val_loss: 0.4605 - val_accuracy: 0.8495\n",
      "Epoch 64/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.6488 - accuracy: 0.7704 - val_loss: 0.4818 - val_accuracy: 0.8466\n",
      "Epoch 65/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 0.6430 - accuracy: 0.7830 - val_loss: 0.4729 - val_accuracy: 0.8523\n",
      "Epoch 66/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.6301 - accuracy: 0.7891 - val_loss: 0.4716 - val_accuracy: 0.8506\n",
      "Epoch 67/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.6378 - accuracy: 0.7771 - val_loss: 0.4540 - val_accuracy: 0.8569\n",
      "Epoch 68/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.6335 - accuracy: 0.7881 - val_loss: 0.4406 - val_accuracy: 0.8580\n",
      "Epoch 69/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.6134 - accuracy: 0.7857 - val_loss: 0.4538 - val_accuracy: 0.8632\n",
      "Epoch 70/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.6432 - accuracy: 0.7838 - val_loss: 0.4509 - val_accuracy: 0.8575\n",
      "Epoch 71/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.6217 - accuracy: 0.7875 - val_loss: 0.4354 - val_accuracy: 0.8661\n",
      "Epoch 72/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.6071 - accuracy: 0.7907 - val_loss: 0.4596 - val_accuracy: 0.8586\n",
      "Epoch 73/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.6271 - accuracy: 0.7827 - val_loss: 0.4278 - val_accuracy: 0.8609\n",
      "Epoch 74/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.6212 - accuracy: 0.7868 - val_loss: 0.4457 - val_accuracy: 0.8672\n",
      "Epoch 75/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.6038 - accuracy: 0.7930 - val_loss: 0.4518 - val_accuracy: 0.8598\n",
      "Epoch 76/100\n",
      "6985/6985 [==============================] - 19s 3ms/step - loss: 0.6064 - accuracy: 0.7920 - val_loss: 0.4294 - val_accuracy: 0.8683\n",
      "Epoch 77/100\n",
      "6985/6985 [==============================] - 19s 3ms/step - loss: 0.5879 - accuracy: 0.8009 - val_loss: 0.4351 - val_accuracy: 0.8666\n",
      "Epoch 78/100\n",
      "6985/6985 [==============================] - 19s 3ms/step - loss: 0.6157 - accuracy: 0.7917 - val_loss: 0.4474 - val_accuracy: 0.8649\n",
      "Epoch 79/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 0.5903 - accuracy: 0.8009 - val_loss: 0.4322 - val_accuracy: 0.8678\n",
      "Epoch 80/100\n",
      "6985/6985 [==============================] - 18s 3ms/step - loss: 0.6004 - accuracy: 0.7947 - val_loss: 0.4273 - val_accuracy: 0.8683\n",
      "Epoch 81/100\n",
      "6985/6985 [==============================] - 19s 3ms/step - loss: 0.5848 - accuracy: 0.8017 - val_loss: 0.4093 - val_accuracy: 0.8804\n",
      "Epoch 82/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.6037 - accuracy: 0.7894 - val_loss: 0.4246 - val_accuracy: 0.8689\n",
      "Epoch 83/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.6180 - accuracy: 0.7867 - val_loss: 0.4342 - val_accuracy: 0.8735\n",
      "Epoch 84/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.6022 - accuracy: 0.7898 - val_loss: 0.4550 - val_accuracy: 0.8632\n",
      "Epoch 85/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.6070 - accuracy: 0.7885 - val_loss: 0.4229 - val_accuracy: 0.8735\n",
      "Epoch 86/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.5887 - accuracy: 0.7973 - val_loss: 0.4386 - val_accuracy: 0.8649\n",
      "Epoch 87/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.5855 - accuracy: 0.7964 - val_loss: 0.4391 - val_accuracy: 0.8695\n",
      "Epoch 88/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.5877 - accuracy: 0.7980 - val_loss: 0.4156 - val_accuracy: 0.8695\n",
      "Epoch 89/100\n",
      "6985/6985 [==============================] - 17s 2ms/step - loss: 0.5769 - accuracy: 0.8006 - val_loss: 0.4088 - val_accuracy: 0.8764\n",
      "Epoch 90/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.5805 - accuracy: 0.7946 - val_loss: 0.4159 - val_accuracy: 0.8741\n",
      "Epoch 91/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 0.5835 - accuracy: 0.8013 - val_loss: 0.4369 - val_accuracy: 0.8643\n",
      "Epoch 92/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.5834 - accuracy: 0.7979 - val_loss: 0.3991 - val_accuracy: 0.8769\n",
      "Epoch 93/100\n",
      "6985/6985 [==============================] - 13s 2ms/step - loss: 0.5583 - accuracy: 0.8077 - val_loss: 0.3974 - val_accuracy: 0.8798\n",
      "Epoch 94/100\n",
      "6985/6985 [==============================] - 15s 2ms/step - loss: 0.5842 - accuracy: 0.7961 - val_loss: 0.3979 - val_accuracy: 0.8832\n",
      "Epoch 95/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 0.5584 - accuracy: 0.8056 - val_loss: 0.4165 - val_accuracy: 0.8752\n",
      "Epoch 96/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 0.5904 - accuracy: 0.7983 - val_loss: 0.4130 - val_accuracy: 0.8786\n",
      "Epoch 97/100\n",
      "6985/6985 [==============================] - 14s 2ms/step - loss: 0.5882 - accuracy: 0.7990 - val_loss: 0.4108 - val_accuracy: 0.8758\n",
      "Epoch 98/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.5699 - accuracy: 0.8027 - val_loss: 0.3997 - val_accuracy: 0.8804\n",
      "Epoch 99/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.5706 - accuracy: 0.8067 - val_loss: 0.4284 - val_accuracy: 0.8695\n",
      "Epoch 100/100\n",
      "6985/6985 [==============================] - 16s 2ms/step - loss: 0.5737 - accuracy: 0.8023 - val_loss: 0.4110 - val_accuracy: 0.8735\n",
      "Training completed in time:  0:26:42.563404\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 64\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 87.3512%\n"
     ]
    }
   ],
   "source": [
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test)\n",
    "accuracy = 100*score[1]\n",
    "print(\"Testing accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 4 gave much better results and hence we have broken the hypothesis of not requiring Deeper neural network instead \n",
    "#we would need wider neural network with more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model -> 6\n",
    "#Construct model \n",
    "#more wider neural network with epochs = 125\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "num_rows = 10\n",
    "n_columns = 4\n",
    "num_channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows,n_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows,n_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, input_shape=(num_rows,n_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6985 samples, validate on 1747 samples\n",
      "Epoch 1/125\n",
      "6985/6985 [==============================] - 29s 4ms/step - loss: 1.1800 - accuracy: 0.5940 - val_loss: 0.9775 - val_accuracy: 0.6892\n",
      "Epoch 2/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 1.0933 - accuracy: 0.6278 - val_loss: 0.8812 - val_accuracy: 0.7247\n",
      "Epoch 3/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 1.0099 - accuracy: 0.6573 - val_loss: 0.8350 - val_accuracy: 0.7252\n",
      "Epoch 4/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.9674 - accuracy: 0.6762 - val_loss: 0.7785 - val_accuracy: 0.7527\n",
      "Epoch 5/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.9135 - accuracy: 0.6896 - val_loss: 0.7726 - val_accuracy: 0.7436\n",
      "Epoch 6/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.8457 - accuracy: 0.7154 - val_loss: 0.6857 - val_accuracy: 0.7871\n",
      "Epoch 7/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.8126 - accuracy: 0.7225 - val_loss: 0.6734 - val_accuracy: 0.7836\n",
      "Epoch 8/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.7892 - accuracy: 0.7340 - val_loss: 0.6236 - val_accuracy: 0.7991\n",
      "Epoch 9/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.7528 - accuracy: 0.7485 - val_loss: 0.5890 - val_accuracy: 0.8140\n",
      "Epoch 10/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.7069 - accuracy: 0.7618 - val_loss: 0.5609 - val_accuracy: 0.8260\n",
      "Epoch 11/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.6902 - accuracy: 0.7682 - val_loss: 0.5191 - val_accuracy: 0.8472\n",
      "Epoch 12/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.6637 - accuracy: 0.7748 - val_loss: 0.5131 - val_accuracy: 0.8397\n",
      "Epoch 13/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.6492 - accuracy: 0.7754 - val_loss: 0.5014 - val_accuracy: 0.8477\n",
      "Epoch 14/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.6002 - accuracy: 0.8009 - val_loss: 0.5079 - val_accuracy: 0.8443\n",
      "Epoch 15/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.5885 - accuracy: 0.7980 - val_loss: 0.4768 - val_accuracy: 0.8517\n",
      "Epoch 16/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.5728 - accuracy: 0.8057 - val_loss: 0.4449 - val_accuracy: 0.8655\n",
      "Epoch 17/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.5540 - accuracy: 0.8076 - val_loss: 0.4685 - val_accuracy: 0.8546\n",
      "Epoch 18/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.5339 - accuracy: 0.8199 - val_loss: 0.4362 - val_accuracy: 0.8666\n",
      "Epoch 19/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.5430 - accuracy: 0.8176 - val_loss: 0.4364 - val_accuracy: 0.8649\n",
      "Epoch 20/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.5135 - accuracy: 0.8239 - val_loss: 0.4150 - val_accuracy: 0.8775\n",
      "Epoch 21/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.4966 - accuracy: 0.8245 - val_loss: 0.3915 - val_accuracy: 0.8752\n",
      "Epoch 22/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.4850 - accuracy: 0.8331 - val_loss: 0.3947 - val_accuracy: 0.8735\n",
      "Epoch 23/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.4858 - accuracy: 0.8331 - val_loss: 0.3910 - val_accuracy: 0.8832\n",
      "Epoch 24/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.4572 - accuracy: 0.8409 - val_loss: 0.3734 - val_accuracy: 0.8815\n",
      "Epoch 25/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.4556 - accuracy: 0.8475 - val_loss: 0.3696 - val_accuracy: 0.8855\n",
      "Epoch 26/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.4597 - accuracy: 0.8404 - val_loss: 0.3492 - val_accuracy: 0.8981\n",
      "Epoch 27/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.4249 - accuracy: 0.8528 - val_loss: 0.3696 - val_accuracy: 0.8849\n",
      "Epoch 28/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.4368 - accuracy: 0.8518 - val_loss: 0.3502 - val_accuracy: 0.8912\n",
      "Epoch 29/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.4093 - accuracy: 0.8578 - val_loss: 0.3665 - val_accuracy: 0.8821\n",
      "Epoch 30/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.4136 - accuracy: 0.8581 - val_loss: 0.3378 - val_accuracy: 0.8947\n",
      "Epoch 31/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.4079 - accuracy: 0.8624 - val_loss: 0.3612 - val_accuracy: 0.8872\n",
      "Epoch 32/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.4063 - accuracy: 0.8608 - val_loss: 0.3411 - val_accuracy: 0.8958\n",
      "Epoch 33/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.4021 - accuracy: 0.8611 - val_loss: 0.3372 - val_accuracy: 0.8890\n",
      "Epoch 34/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.3880 - accuracy: 0.8694 - val_loss: 0.3441 - val_accuracy: 0.8907\n",
      "Epoch 35/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.3875 - accuracy: 0.8686 - val_loss: 0.3305 - val_accuracy: 0.8941\n",
      "Epoch 36/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3736 - accuracy: 0.8713 - val_loss: 0.3218 - val_accuracy: 0.9004\n",
      "Epoch 37/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3663 - accuracy: 0.8729 - val_loss: 0.3408 - val_accuracy: 0.8970\n",
      "Epoch 38/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.3567 - accuracy: 0.8762 - val_loss: 0.3253 - val_accuracy: 0.8981\n",
      "Epoch 39/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3641 - accuracy: 0.8716 - val_loss: 0.3152 - val_accuracy: 0.9067\n",
      "Epoch 40/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3612 - accuracy: 0.8767 - val_loss: 0.3181 - val_accuracy: 0.9067\n",
      "Epoch 41/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3368 - accuracy: 0.8835 - val_loss: 0.2889 - val_accuracy: 0.9107\n",
      "Epoch 42/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3472 - accuracy: 0.8764 - val_loss: 0.3057 - val_accuracy: 0.9090\n",
      "Epoch 43/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.3585 - accuracy: 0.8809 - val_loss: 0.3027 - val_accuracy: 0.9096\n",
      "Epoch 44/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3328 - accuracy: 0.8839 - val_loss: 0.3197 - val_accuracy: 0.9038\n",
      "Epoch 45/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3294 - accuracy: 0.8849 - val_loss: 0.3108 - val_accuracy: 0.9056\n",
      "Epoch 46/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.3275 - accuracy: 0.8826 - val_loss: 0.3063 - val_accuracy: 0.9056\n",
      "Epoch 47/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3337 - accuracy: 0.8829 - val_loss: 0.3301 - val_accuracy: 0.9015\n",
      "Epoch 48/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3278 - accuracy: 0.8888 - val_loss: 0.2857 - val_accuracy: 0.9141\n",
      "Epoch 49/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3159 - accuracy: 0.8902 - val_loss: 0.2991 - val_accuracy: 0.9067\n",
      "Epoch 50/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3093 - accuracy: 0.8956 - val_loss: 0.2954 - val_accuracy: 0.9056\n",
      "Epoch 51/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3204 - accuracy: 0.8893 - val_loss: 0.2907 - val_accuracy: 0.9101\n",
      "Epoch 52/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3097 - accuracy: 0.8945 - val_loss: 0.2976 - val_accuracy: 0.9090\n",
      "Epoch 53/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3037 - accuracy: 0.8969 - val_loss: 0.2862 - val_accuracy: 0.9147\n",
      "Epoch 54/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2976 - accuracy: 0.8992 - val_loss: 0.2777 - val_accuracy: 0.9153\n",
      "Epoch 55/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.3068 - accuracy: 0.8942 - val_loss: 0.2963 - val_accuracy: 0.9056\n",
      "Epoch 56/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.3193 - accuracy: 0.8952 - val_loss: 0.2942 - val_accuracy: 0.9153\n",
      "Epoch 57/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2940 - accuracy: 0.8951 - val_loss: 0.2890 - val_accuracy: 0.9124\n",
      "Epoch 58/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.2890 - accuracy: 0.8985 - val_loss: 0.2952 - val_accuracy: 0.9118\n",
      "Epoch 59/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.2957 - accuracy: 0.8995 - val_loss: 0.2914 - val_accuracy: 0.9141\n",
      "Epoch 60/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.2789 - accuracy: 0.9038 - val_loss: 0.2910 - val_accuracy: 0.9124\n",
      "Epoch 61/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2951 - accuracy: 0.8962 - val_loss: 0.2804 - val_accuracy: 0.9147\n",
      "Epoch 62/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.2966 - accuracy: 0.8955 - val_loss: 0.2720 - val_accuracy: 0.9124\n",
      "Epoch 63/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.2855 - accuracy: 0.9002 - val_loss: 0.2786 - val_accuracy: 0.9159\n",
      "Epoch 64/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2879 - accuracy: 0.9055 - val_loss: 0.2855 - val_accuracy: 0.9153\n",
      "Epoch 65/125\n",
      "6985/6985 [==============================] - 38s 5ms/step - loss: 0.2670 - accuracy: 0.9061 - val_loss: 0.2868 - val_accuracy: 0.9090\n",
      "Epoch 66/125\n",
      "6985/6985 [==============================] - 37s 5ms/step - loss: 0.2717 - accuracy: 0.9097 - val_loss: 0.2865 - val_accuracy: 0.9170\n",
      "Epoch 67/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2749 - accuracy: 0.9035 - val_loss: 0.2823 - val_accuracy: 0.9096\n",
      "Epoch 68/125\n",
      "6985/6985 [==============================] - 26s 4ms/step - loss: 0.2674 - accuracy: 0.9079 - val_loss: 0.2919 - val_accuracy: 0.9090\n",
      "Epoch 69/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.2689 - accuracy: 0.9069 - val_loss: 0.2883 - val_accuracy: 0.9124\n",
      "Epoch 70/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.2601 - accuracy: 0.9108 - val_loss: 0.2985 - val_accuracy: 0.9118\n",
      "Epoch 71/125\n",
      "6985/6985 [==============================] - 23s 3ms/step - loss: 0.2671 - accuracy: 0.9098 - val_loss: 0.2837 - val_accuracy: 0.9164\n",
      "Epoch 72/125\n",
      "6985/6985 [==============================] - 23s 3ms/step - loss: 0.2548 - accuracy: 0.9101 - val_loss: 0.2911 - val_accuracy: 0.9113\n",
      "Epoch 73/125\n",
      "6985/6985 [==============================] - 24s 4ms/step - loss: 0.2611 - accuracy: 0.9104 - val_loss: 0.2794 - val_accuracy: 0.9199\n",
      "Epoch 74/125\n",
      "6985/6985 [==============================] - 23s 3ms/step - loss: 0.2531 - accuracy: 0.9122 - val_loss: 0.2948 - val_accuracy: 0.9199\n",
      "Epoch 75/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2561 - accuracy: 0.9098 - val_loss: 0.2900 - val_accuracy: 0.9136\n",
      "Epoch 76/125\n",
      "6985/6985 [==============================] - 24s 4ms/step - loss: 0.2456 - accuracy: 0.9124 - val_loss: 0.2761 - val_accuracy: 0.9233\n",
      "Epoch 77/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2597 - accuracy: 0.9130 - val_loss: 0.2689 - val_accuracy: 0.9222\n",
      "Epoch 78/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2546 - accuracy: 0.9114 - val_loss: 0.2725 - val_accuracy: 0.9187\n",
      "Epoch 79/125\n",
      "6985/6985 [==============================] - 23s 3ms/step - loss: 0.2662 - accuracy: 0.9091 - val_loss: 0.2678 - val_accuracy: 0.9233\n",
      "Epoch 80/125\n",
      "6985/6985 [==============================] - 23s 3ms/step - loss: 0.2595 - accuracy: 0.9128 - val_loss: 0.2706 - val_accuracy: 0.9187\n",
      "Epoch 81/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2348 - accuracy: 0.9184 - val_loss: 0.2734 - val_accuracy: 0.9170\n",
      "Epoch 82/125\n",
      "6985/6985 [==============================] - 23s 3ms/step - loss: 0.2526 - accuracy: 0.9147 - val_loss: 0.2766 - val_accuracy: 0.9187\n",
      "Epoch 83/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.2475 - accuracy: 0.9142 - val_loss: 0.2770 - val_accuracy: 0.9147\n",
      "Epoch 84/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2368 - accuracy: 0.9155 - val_loss: 0.2919 - val_accuracy: 0.9164\n",
      "Epoch 85/125\n",
      "6985/6985 [==============================] - 23s 3ms/step - loss: 0.2459 - accuracy: 0.9158 - val_loss: 0.2663 - val_accuracy: 0.9159\n",
      "Epoch 86/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.2290 - accuracy: 0.9208 - val_loss: 0.2610 - val_accuracy: 0.9210\n",
      "Epoch 87/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2532 - accuracy: 0.9105 - val_loss: 0.2765 - val_accuracy: 0.9199\n",
      "Epoch 88/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.2498 - accuracy: 0.9137 - val_loss: 0.2946 - val_accuracy: 0.9159\n",
      "Epoch 89/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2309 - accuracy: 0.9217 - val_loss: 0.2728 - val_accuracy: 0.9187\n",
      "Epoch 90/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.2461 - accuracy: 0.9161 - val_loss: 0.2635 - val_accuracy: 0.9210\n",
      "Epoch 91/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2287 - accuracy: 0.9203 - val_loss: 0.2966 - val_accuracy: 0.9136\n",
      "Epoch 92/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2279 - accuracy: 0.9210 - val_loss: 0.2706 - val_accuracy: 0.9193\n",
      "Epoch 93/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.2192 - accuracy: 0.9264 - val_loss: 0.2740 - val_accuracy: 0.9176\n",
      "Epoch 94/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2303 - accuracy: 0.9220 - val_loss: 0.2702 - val_accuracy: 0.9210\n",
      "Epoch 95/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.2378 - accuracy: 0.9195 - val_loss: 0.2808 - val_accuracy: 0.9187\n",
      "Epoch 96/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2267 - accuracy: 0.9246 - val_loss: 0.2797 - val_accuracy: 0.9170\n",
      "Epoch 97/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.2187 - accuracy: 0.9254 - val_loss: 0.2738 - val_accuracy: 0.9187\n",
      "Epoch 98/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2287 - accuracy: 0.9240 - val_loss: 0.2558 - val_accuracy: 0.9250\n",
      "Epoch 99/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2255 - accuracy: 0.9220 - val_loss: 0.2528 - val_accuracy: 0.9193\n",
      "Epoch 100/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.2112 - accuracy: 0.9280 - val_loss: 0.2724 - val_accuracy: 0.9256\n",
      "Epoch 101/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2107 - accuracy: 0.9270 - val_loss: 0.2586 - val_accuracy: 0.9250\n",
      "Epoch 102/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2211 - accuracy: 0.9246 - val_loss: 0.2699 - val_accuracy: 0.9176\n",
      "Epoch 103/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2280 - accuracy: 0.9213 - val_loss: 0.2717 - val_accuracy: 0.9227\n",
      "Epoch 104/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.2181 - accuracy: 0.9266 - val_loss: 0.2660 - val_accuracy: 0.9199\n",
      "Epoch 105/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2247 - accuracy: 0.9241 - val_loss: 0.2847 - val_accuracy: 0.9159\n",
      "Epoch 106/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.2175 - accuracy: 0.9258 - val_loss: 0.2633 - val_accuracy: 0.9273\n",
      "Epoch 107/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.2209 - accuracy: 0.9261 - val_loss: 0.2783 - val_accuracy: 0.9210\n",
      "Epoch 108/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2212 - accuracy: 0.9244 - val_loss: 0.2698 - val_accuracy: 0.9222\n",
      "Epoch 109/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.2339 - accuracy: 0.9225 - val_loss: 0.2797 - val_accuracy: 0.9153\n",
      "Epoch 110/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2094 - accuracy: 0.9283 - val_loss: 0.2740 - val_accuracy: 0.9199\n",
      "Epoch 111/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2015 - accuracy: 0.9291 - val_loss: 0.2687 - val_accuracy: 0.9204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.1919 - accuracy: 0.9323 - val_loss: 0.2711 - val_accuracy: 0.9233\n",
      "Epoch 113/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.1986 - accuracy: 0.9333 - val_loss: 0.2770 - val_accuracy: 0.9222\n",
      "Epoch 114/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2091 - accuracy: 0.9277 - val_loss: 0.2692 - val_accuracy: 0.9256\n",
      "Epoch 115/125\n",
      "6985/6985 [==============================] - 23s 3ms/step - loss: 0.2224 - accuracy: 0.9224 - val_loss: 0.2679 - val_accuracy: 0.9193\n",
      "Epoch 116/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.1929 - accuracy: 0.9351 - val_loss: 0.2769 - val_accuracy: 0.9227\n",
      "Epoch 117/125\n",
      "6985/6985 [==============================] - 23s 3ms/step - loss: 0.2201 - accuracy: 0.9223 - val_loss: 0.2711 - val_accuracy: 0.9256\n",
      "Epoch 118/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2005 - accuracy: 0.9334 - val_loss: 0.2733 - val_accuracy: 0.9222\n",
      "Epoch 119/125\n",
      "6985/6985 [==============================] - 24s 3ms/step - loss: 0.2184 - accuracy: 0.9273 - val_loss: 0.2821 - val_accuracy: 0.9227\n",
      "Epoch 120/125\n",
      "6985/6985 [==============================] - 25s 4ms/step - loss: 0.2009 - accuracy: 0.9319 - val_loss: 0.2576 - val_accuracy: 0.9273\n",
      "Epoch 121/125\n",
      "6985/6985 [==============================] - 29s 4ms/step - loss: 0.2054 - accuracy: 0.9281 - val_loss: 0.2779 - val_accuracy: 0.9193\n",
      "Epoch 122/125\n",
      "6985/6985 [==============================] - 30s 4ms/step - loss: 0.2286 - accuracy: 0.9247 - val_loss: 0.2667 - val_accuracy: 0.9296\n",
      "Epoch 123/125\n",
      "6985/6985 [==============================] - 27s 4ms/step - loss: 0.2065 - accuracy: 0.9310 - val_loss: 0.2619 - val_accuracy: 0.9227\n",
      "Epoch 124/125\n",
      "6985/6985 [==============================] - 29s 4ms/step - loss: 0.2140 - accuracy: 0.9271 - val_loss: 0.2749 - val_accuracy: 0.9256\n",
      "Epoch 125/125\n",
      "6985/6985 [==============================] - 28s 4ms/step - loss: 0.1841 - accuracy: 0.9360 - val_loss: 0.2746 - val_accuracy: 0.9233\n",
      "Training completed in time:  0:54:39.477362\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 125\n",
    "num_batch_size = 64\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  92.3297107219696\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hence more wider the network better the model compared to deeper the neural network\n",
    "#less layers, more hidden nodes with max pooling, reasonable batch size and good amount of epoch will help\n",
    "#achieve better accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
